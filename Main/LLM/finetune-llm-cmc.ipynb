{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11684384,"sourceType":"datasetVersion","datasetId":7333563}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers peft accelerate bitsandbytes torch datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:51:30.162423Z","iopub.execute_input":"2025-05-05T15:51:30.162977Z","iopub.status.idle":"2025-05-05T15:52:52.028528Z","shell.execute_reply.started":"2025-05-05T15:51:30.16295Z","shell.execute_reply":"2025-05-05T15:52:52.02771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\ndef format_data(example):\n\n    template = \"<s>[INST] {prompt} [/INST] {label} </s>\"\n    \n    # Loại bỏ các ký tự \\n không cần thiết trong prompt nếu có để tránh lỗi format\n    prompt_cleaned = example['prompt'].replace('\\n', ' ') \n    return {\"text\": template.format(prompt=prompt_cleaned, label=example['label'])}\n\n# Load dữ liệu gốc\nwith open('/kaggle/input/cmcccc/CMC_dataset_list.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n\n# Chuyển đổi định dạng\nformatted_data = [format_data(item) for item in data]\n\n# Tạo Hugging Face Dataset\ndataset = Dataset.from_list(formatted_data)\n\n# (Optional) Chia train/validation set\ndataset = dataset.train_test_split(test_size=0.2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:03.53434Z","iopub.execute_input":"2025-05-05T15:58:03.535213Z","iopub.status.idle":"2025-05-05T15:58:03.591518Z","shell.execute_reply.started":"2025-05-05T15:58:03.535177Z","shell.execute_reply":"2025-05-05T15:58:03.590798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:07.06599Z","iopub.execute_input":"2025-05-05T15:58:07.066535Z","iopub.status.idle":"2025-05-05T15:58:10.369742Z","shell.execute_reply.started":"2025-05-05T15:58:07.06651Z","shell.execute_reply":"2025-05-05T15:58:10.36881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# --- Bước 0: Đăng nhập vào Hugging Face ---\n# Thay \"YOUR_HF_TOKEN\" bằng API token của bạn từ https://huggingface.co/settings/tokens\nlogin(\"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\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:34.759608Z","iopub.execute_input":"2025-05-05T15:58:34.760011Z","iopub.status.idle":"2025-05-05T15:58:34.907705Z","shell.execute_reply.started":"2025-05-05T15:58:34.759982Z","shell.execute_reply":"2025-05-05T15:58:34.906953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"meta-llama/Meta-Llama-Guard-2-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Set the pad_token\ntokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess(example):\n    return tokenizer(\n        example['text'],\n        truncation=True,\n        max_length=512,\n        padding='max_length'\n    )\n\n# Assuming you have a dataset loaded as 'dataset'\ndataset = dataset.map(preprocess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:36.232249Z","iopub.execute_input":"2025-05-05T15:58:36.23316Z","iopub.status.idle":"2025-05-05T15:58:41.445245Z","shell.execute_reply.started":"2025-05-05T15:58:36.233124Z","shell.execute_reply":"2025-05-05T15:58:41.444455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install git+https://github.com/huggingface/trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:49.430492Z","iopub.execute_input":"2025-05-05T15:58:49.431328Z","iopub.status.idle":"2025-05-05T15:58:59.435827Z","shell.execute_reply.started":"2025-05-05T15:58:49.431303Z","shell.execute_reply":"2025-05-05T15:58:59.434878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes`","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:59.437663Z","iopub.execute_input":"2025-05-05T15:58:59.437894Z","iopub.status.idle":"2025-05-05T15:58:59.581986Z","shell.execute_reply.started":"2025-05-05T15:58:59.437873Z","shell.execute_reply":"2025-05-05T15:58:59.581049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install bitsandbytes accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:58:59.583011Z","iopub.execute_input":"2025-05-05T15:58:59.583252Z","iopub.status.idle":"2025-05-05T15:59:02.716868Z","shell.execute_reply.started":"2025-05-05T15:58:59.58323Z","shell.execute_reply":"2025-05-05T15:59:02.715799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers peft accelerate bitsandbytes torch datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:59:02.718705Z","iopub.execute_input":"2025-05-05T15:59:02.719037Z","iopub.status.idle":"2025-05-05T15:59:06.079998Z","shell.execute_reply.started":"2025-05-05T15:59:02.719015Z","shell.execute_reply":"2025-05-05T15:59:06.07902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom huggingface_hub import login\n\n# --- Bước 0: Đăng nhập vào Hugging Face ---\n# Thay \"YOUR_HF_TOKEN\" bằng API token của bạn từ https://huggingface.co/settings/tokens\nlogin(\"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\") \n\n# --- 1. Cấu hình Quantization (giảm bộ nhớ) ---\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                     # Load model ở dạng 4-bit\n    bnb_4bit_quant_type=\"nf4\",             # Kiểu quantization (nf4 thường hiệu quả)\n    bnb_4bit_compute_dtype=torch.bfloat16, # Kiểu dữ liệu tính toán\n    bnb_4bit_use_double_quant=True,        # Dùng double quantization\n)\n\n# --- 2. Load Model và Tokenizer ---\n# Sử dụng LLaMA Guard model mà bạn đã có quyền truy cập\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  \n\n# Token cache trong transformers sẽ tự động sử dụng token đã đăng nhập\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",                # Tự động phân bổ model lên các GPU có sẵn\n    trust_remote_code=True            # Cần thiết cho một số model\n)\n\nmodel.config.use_cache = False        # Tắt cache để tương thích với gradient checkpointing\nmodel.config.pretraining_tp = 1       # Cần thiết khi dùng TRL với model LLaMA\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Kiểm tra và thiết lập pad_token nếu cần\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token \ntokenizer.padding_side = \"right\"      # Để tránh vấn đề với padding bên trái khi tạo attention mask\n\n# --- 3. Cấu hình LoRA ---\n# Điều chỉnh target_modules cho phù hợp với kiến trúc LLaMA Guard\npeft_config = LoraConfig(\n    r=16,                # Rank của ma trận LoRA\n    lora_alpha=32,       # Alpha scaling (thường gấp đôi r)\n    target_modules=[     # Tên layers cho LLaMA\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\", \n        \"up_proj\", \n        \"down_proj\"\n    ],\n    lora_dropout=0.05,   # Dropout cho LoRA layers\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\", # Loại tác vụ\n)\n\n# Chuẩn bị model cho K-bit training và thêm adapter LoRA\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()    # In ra số lượng tham số cần train (rất nhỏ so với tổng)\n\n# --- 4. Cấu hình Training Arguments ---\ntraining_args = TrainingArguments(\n    output_dir=\"./llama_guard_finetuned\",   # Thư mục lưu kết quả \n    per_device_train_batch_size=2,          # Batch size nhỏ do giới hạn VRAM\n    gradient_accumulation_steps=4,          # Tích lũy gradient để mô phỏng batch size lớn hơn (2*4=8)\n    learning_rate=2e-4,                     # Learning rate thường cao hơn một chút cho LoRA\n    num_train_epochs=1,                     # Số epochs\n    logging_steps=10,                       # Log thông tin training sau mỗi 10 steps\n    save_steps=50,                          # Lưu checkpoint sau mỗi 50 steps\n    fp16=False,                             # Sử dụng mixed precision\n    bf16=True,                              # Sử dụng bfloat16\n    max_grad_norm=0.3,                      # Gradient clipping\n    warmup_ratio=0.03,                      # Tỉ lệ warmup steps\n    lr_scheduler_type=\"constant\",           # Hoặc \"cosine\"\n    report_to=\"tensorboard\",                # Hoặc \"wandb\" nếu dùng\n)\n\n# --- 5. Khởi tạo Trainer ---\n# Xóa tham số tokenizer khỏi SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],          # Dataset đã format ở bước trước\n    eval_dataset=dataset[\"test\"],            # Nếu có validation set\n    peft_config=peft_config,\n    args=training_args,\n    # tokenizer đã bị xóa khỏi đây\n)\n\n# --- 6. Bắt đầu Training ---\nprint(\"Starting training...\")\ntrainer.train()\n\n# --- 7. Lưu Adapter LoRA ---\nprint(\"Saving LoRA adapter...\")\ntrainer.save_model(\"./llama_guard_finetuned/final_adapter\")  # Lưu adapter LoRA\ntokenizer.save_pretrained(\"./llama_guard_finetuned/final_adapter\")\nprint(\"Training finished!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:59:07.751378Z","iopub.execute_input":"2025-05-05T15:59:07.751756Z","iopub.status.idle":"2025-05-05T18:13:13.692704Z","shell.execute_reply.started":"2025-05-05T15:59:07.751729Z","shell.execute_reply":"2025-05-05T18:13:13.691806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Đường dẫn thư mục adapter đã lưu\nfolder_path = \"./llama_guard_finetuned/final_adapter\"\n\n# Nén thành file ZIP\nshutil.make_archive(folder_path, 'zip', folder_path)\n\nprint(\"Zipped successfully! You can now download 'final_adapter.zip'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T18:13:13.694142Z","iopub.execute_input":"2025-05-05T18:13:13.694464Z","iopub.status.idle":"2025-05-05T18:13:22.386469Z","shell.execute_reply.started":"2025-05-05T18:13:13.694436Z","shell.execute_reply":"2025-05-05T18:13:22.385787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nwhile True:\n    print('Awake')\n    time.sleep(10 * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T18:13:22.387158Z","iopub.execute_input":"2025-05-05T18:13:22.387436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport torch\n\n# Load tokenizer and model\nmodel_id = \"microsoft/phi-3-mini-4k-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"  # Dùng GPU nếu có\n)\n\n# Tạo prompt theo kiểu chat\ndef create_prompt(user_input):\n    return f\"<|user|>\\n{user_input}\\n<|assistant|>\"\n\n# Dùng pipeline để sinh văn bản, KHÔNG set max_new_tokens ở đây\ngenerator = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n    pad_token_id=tokenizer.eos_token_id  # tránh lỗi thiếu pad token\n)\n\n# Ví dụ: Tạo prompt từ input người dùng\nuser_input = \"Viết một đoạn văn ngắn giới thiệu về Trái Đất\"\nformatted_prompt = create_prompt(user_input)\n\n# Sinh văn bản, chỉ định max_new_tokens ở đây\nresult = generator(formatted_prompt, max_new_tokens=200)\n\n# In kết quả\nprint(result[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:16:23.935014Z","iopub.execute_input":"2025-05-05T15:16:23.935757Z","iopub.status.idle":"2025-05-05T15:16:37.431345Z","shell.execute_reply.started":"2025-05-05T15:16:23.935734Z","shell.execute_reply":"2025-05-05T15:16:37.430177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft --upgrade\n!pip install transformers --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:21:56.506481Z","iopub.execute_input":"2025-05-05T15:21:56.507081Z","iopub.status.idle":"2025-05-05T15:22:11.007698Z","shell.execute_reply.started":"2025-05-05T15:21:56.507059Z","shell.execute_reply":"2025-05-05T15:22:11.006909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Import PeftModel from peft instead of transformers\nfrom peft import PeftModel\nfrom torch.utils.data import DataLoader\nimport torch\nfrom datasets import Dataset\n\n# --- 1. Chuẩn bị dữ liệu đánh giá từ Dataset đã tiền xử lý ---\n# Giả sử bạn đã có dataset['test'] sau khi chia train/test\neval_dataset = dataset['test']\n\n# Hàm để trích xuất predictions từ logits\ndef extract_predictions(logits):\n    # Điều chỉnh hàm này dựa trên cách mô hình của bạn dự đoán giá trị\n    # Ví dụ, nếu logits trực tiếp là giá trị dự đoán:\n    return logits.squeeze(dim=-1).cpu().numpy()\n    # Hoặc nếu bạn cần lấy token cuối cùng và coi nó là giá trị:\n    # return logits[:, -1, :].argmax(dim=-1).cpu().numpy() # Nếu là phân loại token\n    # Hoặc một phương pháp khác tùy thuộc vào mô hình của bạn\n\n# Hàm để trích xuất ground truth từ labels\ndef extract_ground_truth(labels):\n    # Giả định labels là tensor chứa giá trị số\n    return labels.squeeze(dim=-1).cpu().numpy()\n\n# --- 2. Tải mô hình và tokenizer đã tinh chỉnh ---\nadapter_path = \"./cmc_llama_finetuned/final_adapter\"\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\" # Đảm bảo đây là tên chính xác\n\ntokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\nmodel = PeftModel.from_pretrained(model, adapter_path)\nmodel.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- 3. Cấu hình DataLoader cho dữ liệu đánh giá ---\neval_dataloader = DataLoader(eval_dataset, batch_size=8) # Điều chỉnh batch size nếu cần\n\n# --- 4. Thực hiện đánh giá ---\npredictions = []\nground_truth = []\n\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        input_ids = batch['input_ids'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        attention_mask = batch['attention_mask'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        labels = batch['labels'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Lấy logits từ output\n        logits = outputs.logits\n\n        # Trích xuất giá trị dự đoán và ground truth\n        batch_predictions = extract_predictions(logits)\n        batch_ground_truth = extract_ground_truth(labels)\n\n        predictions.extend(batch_predictions)\n        ground_truth.extend(batch_ground_truth)\n\n# Chuyển về numpy arrays để tính toán metrics\npredictions = np.array(predictions)\nground_truth = np.array(ground_truth)\n\n# --- 5. Tính toán và in ra các metrics đánh giá ---\nmse = mean_squared_error(ground_truth, predictions)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(ground_truth, predictions)\nr_squared = r2_score(ground_truth, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"R-squared: {r_squared:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:28:27.812333Z","iopub.execute_input":"2025-05-05T15:28:27.812915Z","iopub.status.idle":"2025-05-05T15:28:39.997167Z","shell.execute_reply.started":"2025-05-05T15:28:27.812893Z","shell.execute_reply":"2025-05-05T15:28:39.996059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install peft\n!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:23:43.932961Z","iopub.execute_input":"2025-05-05T15:23:43.933479Z","iopub.status.idle":"2025-05-05T15:25:52.408499Z","shell.execute_reply.started":"2025-05-05T15:23:43.933458Z","shell.execute_reply":"2025-05-05T15:25:52.407495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}