import pandas as pd
import numpy as np
df=pd.read_csv(r"C:\Users\Admin\Downloads\all_news_20250426 (1).csv")

import pandas as pd
df['date'] = pd.to_datetime(df['date'])
#s·∫Øp x·∫øp theo c·ªôt date
df = df.sort_values(by='date')
import pandas as pd
import numpy as np

# Assuming df and the 'source' column already exist from previous code
df['source'] = df['source'].str.replace('/rss', '', regex=False)
#reset index
df = df.reset_index(drop=True)

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# --- H√ÄM T·∫†O CHROME DRIVER ---
def create_driver():
    options = Options()
    options.add_argument("--headless=new")  # headless ch·∫ø ƒë·ªô m·ªõi
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

# --- H√ÄM L·∫§Y N·ªòI DUNG B√ÄI B√ÅO ---
def get_real_article_content(driver, news_google_url):
    try:
        driver.get(news_google_url)
        time.sleep(random.uniform(2, 4))  # Random th·ªùi gian ch·ªù

        real_article_url = driver.current_url
        print(f"‚úÖ Link b√†i b√°o g·ªëc: {real_article_url}")

        if "news.google.com" in real_article_url:
            print("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c link b√†i b√°o g·ªëc.")
            return None

        driver.get(real_article_url)
        time.sleep(random.uniform(2, 4))

        soup = BeautifulSoup(driver.page_source, "html.parser")

        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()

        full_text = soup.get_text(separator="\n", strip=True)

        return full_text

    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y b√†i b√°o: {e}")
        return None


df['content'] = pd.Series(dtype=str)

# --- CH·∫†Y QU√âT D·ªÆ LI·ªÜU ---
driver = create_driver()

for i in range(len(df)):
    try:
        if pd.isnull(df.loc[i, 'content']):
            url = df.loc[i, 'source']
            full_text = get_real_article_content(driver, url)

            if full_text:
                df.loc[i, 'content'] = full_text
                print(f"‚úÖ ƒê√£ l∆∞u n·ªôi dung b√†i {i}")

            else:
                print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung b√†i {i}")

            # Pause random m·ªói b√†i
            time.sleep(random.uniform(5, 10))

            # Restart driver m·ªói 5 b√†i
            if (i + 1) % 5 == 0:
                driver.quit()
                print("üîÑ Restart driver...")
                time.sleep(random.uniform(5, 8))
                driver = create_driver()

    except Exception as e:
        print(f"‚ùå L·ªói ·ªü b√†i {i}: {e}")
        continue  # B·ªè qua l·ªói, ch·∫°y ti·∫øp

driver.quit()

print(df)
df.to_csv('outputnews.csv', index=False)  # Save n·∫øu c·∫ßn
