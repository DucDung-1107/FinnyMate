{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11623851,"sourceType":"datasetVersion","datasetId":7292220},{"sourceId":11721200,"sourceType":"datasetVersion","datasetId":7357981},{"sourceId":11721317,"sourceType":"datasetVersion","datasetId":7358055},{"sourceId":237080332,"sourceType":"kernelVersion"},{"sourceId":383188,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":316318,"modelId":336832}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install pandas faiss-cpu sentence-transformers openai"],"metadata":{"id":"aRtUtdd9kqbn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1d0f5ed-b8ba-4f32-a291-84562b593cf8","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:16:50.314167Z","iopub.execute_input":"2025-05-09T03:16:50.314949Z","iopub.status.idle":"2025-05-09T03:18:07.260183Z","shell.execute_reply.started":"2025-05-09T03:16:50.314921Z","shell.execute_reply":"2025-05-09T03:18:07.259459Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, faiss-cpu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Đọc và tiền xử lý dữ liệu tin tức từ các nguồn"],"metadata":{"id":"AR9uffumn69R"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import json\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import openai\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","def clean_text(text):\n","    if pd.isna(text):\n","        return \"\"\n","    text = re.sub(r'<[^>]+>', '', str(text))\n","    text = re.sub(r'<[^>]+>|[\\*\\#\\@]', '', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    return text.strip()\n","\n","def extract_ticker(text):\n","    text = str(text).upper()\n","    # Thêm các pattern phức tạp hơn và xử lý viết tắt\n","    patterns = [\n","        r'\\b(FPT|CMG)\\b',\n","        r'\\b(FPT\\d*[A-Z]*)\\b',\n","        r'\\b(CMG\\d*[A-Z]*)\\b'\n","    ]\n","    for pattern in patterns:\n","        match = re.search(pattern, text)\n","        if match:\n","            return match.group(0)\n","    return 'UNKNOWN'\n","\n","def preprocess_news(df, source_label):\n","    title_col = 'title' if 'title' in df.columns else df.columns[0]\n","    content_col = 'summary' if 'summary' in df.columns else df.columns[1]\n","\n","    # Cột ngày\n","    if 'date' in df.columns:\n","        date_col = 'date'\n","    else:\n","        possible_date = [col for col in df.columns if \"date\" in col.lower() or \"ngày\" in col.lower()]\n","        if possible_date:\n","            date_col = possible_date[0]\n","        else:\n","            raise ValueError(f\"Không tìm thấy cột ngày trong DataFrame {source_label}\")\n","\n","    # Làm sạch\n","    df['title'] = df[title_col].apply(clean_text)\n","    df['content'] = df[content_col].apply(clean_text)\n","    df['text'] = df['title'] + \". \" + df['content']\n","\n","    # Parse ngày: mặc định mm/dd/yyyy → dayfirst=False\n","    df['date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=False)\n","\n","    df['source'] = source_label\n","    df['record_date'] = df['date']\n","\n","    ticker_col = 'ticker' if 'ticker' in df.columns else None\n","\n","    # Nếu có ticker thì dùng, không thì trích\n","    if ticker_col:\n","        df['ticker'] = df[ticker_col]\n","    else:\n","        df['ticker'] = df['text'].apply(extract_ticker)\n","\n","    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n","\n","def process_divided(df, source_label):\n","\n","    # Đầu tiên, chuẩn hóa tên cột về dạng dễ xử lý nếu cần\n","    df.columns = df.columns.str.strip().str.lower()\n","\n","    # Đổi tên cho dễ code\n","    rename_mapping = {\n","        'exchange': 'exchange',\n","        'ex-dividend date': 'ex_dividend_date',\n","        'record date': 'record_date',\n","        'execution date': 'execution_date',\n","        'event content': 'event_content',\n","        'event type': 'event_type'\n","    }\n","    df = df.rename(columns=rename_mapping)\n","\n","    # Tạo cột mới gộp thông tin\n","    def combine_event_info(row):\n","        parts = []\n","        parts.append(f\"Sàn giao dịch: {row['exchange'] if pd.notna(row['exchange']) and row['exchange'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày giao dịch không hưởng quyền: {row['ex_dividend_date'] if pd.notna(row['ex_dividend_date']) and row['ex_dividend_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày chốt danh sách: {row['record_date'] if pd.notna(row['record_date']) and row['record_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày thực hiện: {row['execution_date'] if pd.notna(row['execution_date']) and row['execution_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Nội dung sự kiện: {row['event_content'] if pd.notna(row['event_content']) and row['event_content'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Loại sự kiện: {row['event_type'] if pd.notna(row['event_type']) and row['event_type'] else 'UNKNOWN'}.\")\n","        return \" \".join(parts)\n","\n","    df['text'] = df.apply(combine_event_info, axis=1)\n","    df['date'] = pd.to_datetime(df['execution_date'], errors='coerce', dayfirst=True)\n","    df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce', dayfirst=True)\n","\n","    df['source'] = source_label\n","    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n","    # df[['record_date', 'date', 'ticker', 'text', 'source']]\n","\n","    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n","\n","def process_shareholder(df, source_label):\n","\n","    # Đầu tiên, chuẩn hóa tên cột về dạng dễ xử lý nếu cần\n","    df.columns = df.columns.str.strip().str.lower()\n","\n","    # Đổi tên cho dễ code\n","    rename_mapping = {\n","        'exchange': 'exchange',\n","        'ex-rights date': 'ex_rights_date',\n","        'record date': 'record_date',\n","        'execution date': 'execution_date',\n","        'event type': 'event_type'\n","    }\n","    df = df.rename(columns=rename_mapping)\n","\n","    # Tạo cột mới gộp thông tin\n","    def combine_event_info(row):\n","        parts = []\n","        parts.append(f\"Sàn giao dịch: {row['exchange'] if pd.notna(row['exchange']) and row['exchange'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày giao dịch không hưởng quyền: {row['ex_rights_date'] if pd.notna(row['ex_rights_date']) and row['ex_rights_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày chốt danh sách: {row['record_date'] if pd.notna(row['record_date']) and row['record_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày thực hiện: {row['execution_date'] if pd.notna(row['execution_date']) and row['execution_date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Loại sự kiện: {row['event_type'] if pd.notna(row['event_type']) and row['event_type'] else 'UNKNOWN'}.\")\n","        return \" \".join(parts)\n","\n","\n","    df['text'] = df.apply(combine_event_info, axis=1)\n","    df['date'] = pd.to_datetime(df['execution_date'], errors='coerce', dayfirst=True)\n","    df['record_date'] = pd.to_datetime(df['record_date'], errors='coerce', dayfirst=True)\n","    df['source'] = source_label\n","    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n","\n","    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n","\n","\n","def process_internal(df, source_label):\n","    # Chuẩn hóa tên cột\n","    df.columns = df.columns.str.strip().str.lower()\n","\n","    # Các cột bạn muốn gộp\n","    columns_to_combine = [\n","        'transaction type', 'executor name', 'executor position', 'related person name',\n","        'related person position', 'relation', 'before transaction volume', 'before transaction percentage',\n","        'registered transaction volume', 'registered from date', 'registered to date',\n","        'executed transaction volume', 'executed from date', 'executed to date',\n","        'after transaction volume', 'after transaction percentage'\n","    ]\n","\n","    for col in columns_to_combine:\n","        if col in df.columns:\n","            df[col] = df[col].apply(clean_text)\n","\n","    # Hàm gộp thành text\n","    def combine_fields(row):\n","        parts = []\n","        parts.append(f\"Loại giao dịch: {row['transaction type'] if pd.notna(row['transaction type']) and row['transaction type'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Người thực hiện: {row['executor name'] if pd.notna(row['executor name']) and row['executor name'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Chức vụ người thực hiện: {row['executor position'] if pd.notna(row['executor position']) and row['executor position'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Người liên quan: {row['related person name'] if pd.notna(row['related person name']) and row['related person name'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Chức vụ người liên quan: {row['related person position'] if pd.notna(row['related person position']) and row['related person position'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Quan hệ: {row['relation'] if pd.notna(row['relation']) and row['relation'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Số lượng trước giao dịch: {row['before transaction volume'] if pd.notna(row['before transaction volume']) and row['before transaction volume'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Tỷ lệ trước giao dịch: {row['before transaction percentage'] if pd.notna(row['before transaction percentage']) and row['before transaction percentage'] else 'UNKNOWN'}%.\")\n","        parts.append(f\"Số lượng đăng ký: {row['registered transaction volume'] if pd.notna(row['registered transaction volume']) and row['registered transaction volume'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày bắt đầu đăng ký: {row['registered from date'] if pd.notna(row['registered from date']) and row['registered from date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày kết thúc đăng ký: {row['registered to date'] if pd.notna(row['registered to date']) and row['registered to date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Số lượng thực tế giao dịch: {row['executed transaction volume'] if pd.notna(row['executed transaction volume']) and row['executed transaction volume'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày bắt đầu thực hiện: {row['executed from date'] if pd.notna(row['executed from date']) and row['executed from date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Ngày kết thúc thực hiện: {row['executed to date'] if pd.notna(row['executed to date']) and row['executed to date'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Số lượng sau giao dịch: {row['after transaction volume'] if pd.notna(row['after transaction volume']) and row['after transaction volume'] else 'UNKNOWN'}.\")\n","        parts.append(f\"Tỷ lệ sau giao dịch: {row['after transaction percentage'] if pd.notna(row['after transaction percentage']) and row['after transaction percentage'] else 'UNKNOWN'}%.\")\n","        return \" \".join(parts)\n","\n","\n","    # Tạo cột text\n","    df['text'] = df.apply(combine_fields, axis=1)\n","    df['date'] = pd.to_datetime(df['executed to date'], errors='coerce', dayfirst=True)\n","    df['record_date'] = pd.to_datetime(df['executed from date'], errors='coerce', dayfirst=True)\n","    df['source'] = source_label\n","    df['ticker'] = df['stockid'] if 'stockid' in df.columns else 'UNKNOWN'\n","\n","    return df[['record_date', 'date', 'ticker', 'text', 'source']]\n","\n","\n","\n","#Đọc file từ các nguồn\n","df_cafef = pd.read_excel(\"/kaggle/input/barefoots/CafeF_News_FPT_CM1.xlsx\")\n","df_dividend = pd.read_excel(\"/kaggle/input/barefoots/32news_dividend_issue FPT_CMG_processed.xlsx\")\n","df_shareholder = pd.read_excel(\"/kaggle/input/barefoots/33news_shareholder_meetingFPT_CMG_processed.xlsx\")\n","df_internal = pd.read_csv(\"/kaggle/input/barefoots/34news_internal_transactionsFPT_CMG_processed.csv\")\n","\n","#Tiền xử lý từng DataFrame\n","df_cafef_clean = preprocess_news(df_cafef, \"cafef\").fillna(\"UNKNOWN\")\n","df_dividend_clean = process_divided(df_dividend, \"dividend\").fillna(\"UNKNOWN\")\n","df_shareholder_clean = process_shareholder(df_shareholder, \"shareholder\").fillna(\"UNKNOWN\")\n","df_internal_clean = process_internal(df_internal, \"internal\").fillna(\"UNKNOWN\")\n","\n","# Gộp\n","df_all_news = pd.concat([\n","    df_cafef_clean, df_dividend_clean, df_shareholder_clean, df_internal_clean\n","], ignore_index=True)\n","\n","\n","#Hợp nhất tất cả dữ liệu tin tức\n","df_all_news = pd.concat([\n","    df_cafef_clean, df_dividend_clean, df_shareholder_clean, df_internal_clean\n","], ignore_index=True)\n","\n","print(df_all_news)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:18:07.261533Z","iopub.execute_input":"2025-05-09T03:18:07.261780Z","iopub.status.idle":"2025-05-09T03:18:35.617596Z","shell.execute_reply.started":"2025-05-09T03:18:07.261759Z","shell.execute_reply":"2025-05-09T03:18:35.616949Z"},"id":"4MpGuwb9VLLd","outputId":"94ba0d87-726c-407d-d40e-73b6a95995de"},"outputs":[{"name":"stderr","text":"2025-05-09 03:18:20.213958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746760700.398551      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746760700.449151      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"             record_date                 date ticker  \\\n0    2025-03-12 00:00:00  2025-03-12 00:00:00    FPT   \n1    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n2    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n3    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n4    2025-03-11 00:00:00  2025-03-11 00:00:00    FPT   \n..                   ...                  ...    ...   \n982  2023-05-22 00:00:00  2023-06-20 00:00:00    CMG   \n983  2023-04-26 00:00:00  2023-05-25 00:00:00    CMG   \n984  2023-03-23 00:00:00  2023-04-21 00:00:00    CMG   \n985  2023-03-15 00:00:00  2023-04-13 00:00:00    CMG   \n986  2023-03-14 00:00:00  2023-03-14 00:00:00    CMG   \n\n                                                  text    source  \n0    Phiên 12/3: Khối ngoại bán chiến biến hơn 900 ...     cafef  \n1    Chứng minh ngày mai (12-3): VN-Index tiếp tục ...     cafef  \n2    FPT \"Bắt tay\" Tỉnh Bắc Giang phát triển toàn d...     cafef  \n3    CTCK tự doanh không mong đợi trở lại \"gom\" một...     cafef  \n4    Chứng chỉ thoát hiểm 'phút 89'. Tóm tắt thị tr...     cafef  \n..                                                 ...       ...  \n982  Loại giao dịch: GD của người liên quan. Người ...  internal  \n983  Loại giao dịch: GD của người liên quan. Người ...  internal  \n984  Loại giao dịch: GD của người liên quan. Người ...  internal  \n985  Loại giao dịch: GD của người liên quan. Người ...  internal  \n986  Loại giao dịch: GD CĐ lớn. Người thực hiện: Py...  internal  \n\n[987 rows x 5 columns]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Chunking"],"metadata":{"id":"iTnPCzp9oBWM"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import pandas as pd\n","\n","# Thiết lập bộ chunking\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=500,\n","    chunk_overlap=50,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",")\n","\n","# Áp dụng chunking vào từng dòng trong df_all_news\n","chunks = []\n","\n","for idx, row in df_all_news.iterrows():\n","    split_texts = text_splitter.split_text(row['text'])\n","    for chunk_text in split_texts:\n","        chunks.append({\n","            \"text\": chunk_text + \" .Ngày: \" + str(row['date']) + \" .Công ty:\" + str(row['ticker']),\n","            \"ticker\": row['ticker'],\n","            \"record_date\": row['record_date'],\n","            \"date\": row['date'],\n","            \"source\": row['source']\n","        })\n","\n","df_chunks = pd.DataFrame(chunks)\n","\n","# Kết quả\n","print(f\"Số lượng chunk tạo ra: {len(df_chunks)}\")\n","print(df_chunks.head())\n","df_chunks = df_chunks[df_chunks['text'].notna() & df_chunks['text'].str.strip().ne(\"\")]"],"metadata":{"id":"rfZQpuyqjRsB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e80d2a5f-3a48-4b99-e85e-d0fde3122501","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:18:35.618416Z","iopub.execute_input":"2025-05-09T03:18:35.618841Z","iopub.status.idle":"2025-05-09T03:18:36.385870Z","shell.execute_reply.started":"2025-05-09T03:18:35.618811Z","shell.execute_reply":"2025-05-09T03:18:36.385207Z"}},"outputs":[{"name":"stdout","text":"Số lượng chunk tạo ra: 4272\n                                                text ticker  \\\n0  Phiên 12/3: Khối ngoại bán chiến biến hơn 900 ...    FPT   \n1  . Hoạt động giao dịch của khối ngoại: Khối ngo...    FPT   \n2  . Tổng quan HNX và UPCOM: Trên HNX, khối ngoại...    FPT   \n3  . Nhìn chung, trong khi VN-Index cho thấy khả ...    FPT   \n4  Chứng minh ngày mai (12-3): VN-Index tiếp tục ...    FPT   \n\n           record_date                 date source  \n0  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n1  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n2  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n3  2025-03-12 00:00:00  2025-03-12 00:00:00  cafef  \n4  2025-03-11 00:00:00  2025-03-11 00:00:00  cafef  \n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# embedding"],"metadata":{"id":"wa5ahH5poSgq"}},{"cell_type":"code","source":["# login vào huggingface\n","from huggingface_hub import login\n","login(token='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')"],"metadata":{"id":"ln326QHybokn","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:18:36.387294Z","iopub.execute_input":"2025-05-09T03:18:36.387513Z","iopub.status.idle":"2025-05-09T03:18:36.531556Z","shell.execute_reply.started":"2025-05-09T03:18:36.387495Z","shell.execute_reply":"2025-05-09T03:18:36.530865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Tải mô hình PhoBERT\n","embedding_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","embedding_model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","embedding_model = embedding_model.to(device)\n","\n","# Hàm mean pooling\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0]  # (batch_size, seq_len, hidden_size)\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","    return sum_embeddings / sum_mask\n","\n","# Hàm encode toàn bộ df_chunks['text']\n","def encode_phobert(texts):\n","    embeddings = []\n","    for text in tqdm(texts, desc=\"Encoding with PhoBERT\"):\n","        encoded_input = embedding_tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=512)\n","        # Move encoded_input to the same device as the model\n","        encoded_input = encoded_input.to(device) # This line has been added to move the input to the GPU\n","        with torch.no_grad():\n","            model_output = embedding_model(**encoded_input)\n","        sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n","        embeddings.append(sentence_embedding.squeeze(0).cpu().numpy())\n","    return np.vstack(embeddings)\n","\n","# Dùng để embedding\n","texts = df_chunks['text'].tolist()\n","embeddings = encode_phobert(texts)\n","\n","print(\"Embedding shape:\", embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358,"referenced_widgets":["1c51eeabecd24efb96b417fbff12aa69","bb3d788d619240f8994fc58dc996fcd9","bc955aa201a14a0c9b5df1d8226a3843","1478ee73318944869a73d231996b0c90","db858f6cdc864719bf8d005c23078db7","28274a401f014b869ca2d5de11ed704e"]},"id":"QxSKkwS9Hydd","outputId":"484e2270-c97f-4449-f2da-a678e2b85bfc","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:18:36.532443Z","iopub.execute_input":"2025-05-09T03:18:36.532713Z","iopub.status.idle":"2025-05-09T03:19:28.303119Z","shell.execute_reply.started":"2025-05-09T03:18:36.532689Z","shell.execute_reply":"2025-05-09T03:19:28.302537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c51eeabecd24efb96b417fbff12aa69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb3d788d619240f8994fc58dc996fcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc955aa201a14a0c9b5df1d8226a3843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1478ee73318944869a73d231996b0c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db858f6cdc864719bf8d005c23078db7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28274a401f014b869ca2d5de11ed704e"}},"metadata":{}},{"name":"stderr","text":"\nEncoding with PhoBERT:   0%|          | 0/4272 [00:00<?, ?it/s]\u001b[A\nEncoding with PhoBERT:   0%|          | 1/4272 [00:00<27:06,  2.63it/s]\u001b[A\nEncoding with PhoBERT:   0%|          | 8/4272 [00:00<03:27, 20.50it/s]\u001b[A\nEncoding with PhoBERT:   0%|          | 13/4272 [00:00<02:28, 28.60it/s]\u001b[A\nEncoding with PhoBERT:   0%|          | 20/4272 [00:00<01:49, 38.89it/s]\u001b[A\nEncoding with PhoBERT:   1%|          | 28/4272 [00:00<01:26, 48.87it/s]\u001b[A\nEncoding with PhoBERT:   1%|          | 36/4272 [00:00<01:14, 56.92it/s]\u001b[A\nEncoding with PhoBERT:   1%|          | 45/4272 [00:01<01:06, 64.00it/s]\u001b[A\nEncoding with PhoBERT:   1%|▏         | 54/4272 [00:01<00:59, 70.62it/s]\u001b[A\nEncoding with PhoBERT:   1%|▏         | 62/4272 [00:01<01:07, 62.42it/s]\u001b[A\nEncoding with PhoBERT:   2%|▏         | 69/4272 [00:01<01:06, 63.27it/s]\u001b[A\nEncoding with PhoBERT:   2%|▏         | 78/4272 [00:01<01:01, 68.32it/s]\u001b[A\nEncoding with PhoBERT:   2%|▏         | 87/4272 [00:01<00:57, 72.24it/s]\u001b[A\nEncoding with PhoBERT:   2%|▏         | 95/4272 [00:01<01:02, 67.03it/s]\u001b[A\nEncoding with PhoBERT:   2%|▏         | 103/4272 [00:01<01:00, 68.78it/s]\u001b[A\nEncoding with PhoBERT:   3%|▎         | 111/4272 [00:01<01:00, 68.81it/s]\u001b[A\nEncoding with PhoBERT:   3%|▎         | 118/4272 [00:02<01:00, 69.07it/s]\u001b[A\nEncoding with PhoBERT:   3%|▎         | 129/4272 [00:02<00:53, 77.98it/s]\u001b[A\nEncoding with PhoBERT:   3%|▎         | 140/4272 [00:02<00:48, 85.06it/s]\u001b[A\nEncoding with PhoBERT:   4%|▎         | 151/4272 [00:02<00:45, 90.57it/s]\u001b[A\nEncoding with PhoBERT:   4%|▍         | 162/4272 [00:02<00:43, 94.21it/s]\u001b[A\nEncoding with PhoBERT:   4%|▍         | 173/4272 [00:02<00:42, 97.54it/s]\u001b[A\nEncoding with PhoBERT:   4%|▍         | 183/4272 [00:02<00:41, 97.40it/s]\u001b[A\nEncoding with PhoBERT:   5%|▍         | 194/4272 [00:02<00:41, 98.66it/s]\u001b[A\nEncoding with PhoBERT:   5%|▍         | 205/4272 [00:02<00:40, 101.08it/s]\u001b[A\nEncoding with PhoBERT:   5%|▌         | 216/4272 [00:03<00:40, 100.22it/s]\u001b[A\nEncoding with PhoBERT:   5%|▌         | 227/4272 [00:03<00:39, 102.37it/s]\u001b[A\nEncoding with PhoBERT:   6%|▌         | 238/4272 [00:03<00:38, 104.45it/s]\u001b[A\nEncoding with PhoBERT:   6%|▌         | 249/4272 [00:03<00:38, 104.42it/s]\u001b[A\nEncoding with PhoBERT:   6%|▌         | 260/4272 [00:03<00:38, 103.59it/s]\u001b[A\nEncoding with PhoBERT:   6%|▋         | 271/4272 [00:03<00:38, 105.23it/s]\u001b[A\nEncoding with PhoBERT:   7%|▋         | 282/4272 [00:03<00:38, 104.60it/s]\u001b[A\nEncoding with PhoBERT:   7%|▋         | 293/4272 [00:03<00:38, 104.38it/s]\u001b[A\nEncoding with PhoBERT:   7%|▋         | 305/4272 [00:03<00:36, 107.76it/s]\u001b[A\nEncoding with PhoBERT:   7%|▋         | 316/4272 [00:03<00:36, 107.16it/s]\u001b[A\nEncoding with PhoBERT:   8%|▊         | 327/4272 [00:04<00:37, 106.45it/s]\u001b[A\nEncoding with PhoBERT:   8%|▊         | 338/4272 [00:04<00:37, 105.55it/s]\u001b[A\nEncoding with PhoBERT:   8%|▊         | 349/4272 [00:04<00:36, 106.31it/s]\u001b[A\nEncoding with PhoBERT:   8%|▊         | 360/4272 [00:04<00:36, 107.07it/s]\u001b[A\nEncoding with PhoBERT:   9%|▊         | 371/4272 [00:04<00:36, 107.40it/s]\u001b[A\nEncoding with PhoBERT:   9%|▉         | 383/4272 [00:04<00:35, 109.21it/s]\u001b[A\nEncoding with PhoBERT:   9%|▉         | 394/4272 [00:04<00:35, 108.64it/s]\u001b[A\nEncoding with PhoBERT:  10%|▉         | 406/4272 [00:04<00:35, 109.59it/s]\u001b[A\nEncoding with PhoBERT:  10%|▉         | 418/4272 [00:04<00:34, 110.21it/s]\u001b[A\nEncoding with PhoBERT:  10%|█         | 430/4272 [00:05<00:34, 110.16it/s]\u001b[A\nEncoding with PhoBERT:  10%|█         | 442/4272 [00:05<00:36, 106.25it/s]\u001b[A\nEncoding with PhoBERT:  11%|█         | 453/4272 [00:05<00:35, 106.28it/s]\u001b[A\nEncoding with PhoBERT:  11%|█         | 465/4272 [00:05<00:34, 109.11it/s]\u001b[A\nEncoding with PhoBERT:  11%|█         | 477/4272 [00:05<00:34, 109.24it/s]\u001b[A\nEncoding with PhoBERT:  11%|█▏        | 489/4272 [00:05<00:34, 110.58it/s]\u001b[A\nEncoding with PhoBERT:  12%|█▏        | 501/4272 [00:05<00:34, 108.43it/s]\u001b[A\nEncoding with PhoBERT:  12%|█▏        | 512/4272 [00:05<00:35, 107.37it/s]\u001b[A\nEncoding with PhoBERT:  12%|█▏        | 524/4272 [00:05<00:34, 108.29it/s]\u001b[A\nEncoding with PhoBERT:  13%|█▎        | 536/4272 [00:06<00:34, 109.17it/s]\u001b[A\nEncoding with PhoBERT:  13%|█▎        | 548/4272 [00:06<00:33, 111.51it/s]\u001b[A\nEncoding with PhoBERT:  13%|█▎        | 560/4272 [00:06<00:34, 109.14it/s]\u001b[A\nEncoding with PhoBERT:  13%|█▎        | 571/4272 [00:06<00:33, 109.21it/s]\u001b[A\nEncoding with PhoBERT:  14%|█▎        | 582/4272 [00:06<00:34, 107.17it/s]\u001b[A\nEncoding with PhoBERT:  14%|█▍        | 593/4272 [00:06<00:34, 106.19it/s]\u001b[A\nEncoding with PhoBERT:  14%|█▍        | 604/4272 [00:06<00:34, 105.29it/s]\u001b[A\nEncoding with PhoBERT:  14%|█▍        | 616/4272 [00:06<00:33, 108.78it/s]\u001b[A\nEncoding with PhoBERT:  15%|█▍        | 628/4272 [00:06<00:33, 109.86it/s]\u001b[A\nEncoding with PhoBERT:  15%|█▍        | 639/4272 [00:06<00:33, 108.97it/s]\u001b[A\nEncoding with PhoBERT:  15%|█▌        | 651/4272 [00:07<00:32, 111.16it/s]\u001b[A\nEncoding with PhoBERT:  16%|█▌        | 663/4272 [00:07<00:32, 109.52it/s]\u001b[A\nEncoding with PhoBERT:  16%|█▌        | 674/4272 [00:07<00:33, 106.38it/s]\u001b[A\nEncoding with PhoBERT:  16%|█▌        | 685/4272 [00:07<00:33, 105.67it/s]\u001b[A\nEncoding with PhoBERT:  16%|█▋        | 696/4272 [00:07<00:33, 105.48it/s]\u001b[A\nEncoding with PhoBERT:  17%|█▋        | 707/4272 [00:07<00:33, 106.07it/s]\u001b[A\nEncoding with PhoBERT:  17%|█▋        | 718/4272 [00:07<00:33, 106.65it/s]\u001b[A\nEncoding with PhoBERT:  17%|█▋        | 730/4272 [00:07<00:32, 108.13it/s]\u001b[A\nEncoding with PhoBERT:  17%|█▋        | 741/4272 [00:07<00:33, 106.74it/s]\u001b[A\nEncoding with PhoBERT:  18%|█▊        | 752/4272 [00:08<00:32, 107.40it/s]\u001b[A\nEncoding with PhoBERT:  18%|█▊        | 763/4272 [00:08<00:32, 108.15it/s]\u001b[A\nEncoding with PhoBERT:  18%|█▊        | 774/4272 [00:08<00:32, 106.39it/s]\u001b[A\nEncoding with PhoBERT:  18%|█▊        | 785/4272 [00:08<00:32, 106.76it/s]\u001b[A\nEncoding with PhoBERT:  19%|█▊        | 797/4272 [00:08<00:32, 108.12it/s]\u001b[A\nEncoding with PhoBERT:  19%|█▉        | 809/4272 [00:08<00:31, 110.98it/s]\u001b[A\nEncoding with PhoBERT:  19%|█▉        | 821/4272 [00:08<00:30, 112.25it/s]\u001b[A\nEncoding with PhoBERT:  19%|█▉        | 833/4272 [00:08<00:31, 109.45it/s]\u001b[A\nEncoding with PhoBERT:  20%|█▉        | 844/4272 [00:08<00:31, 109.56it/s]\u001b[A\nEncoding with PhoBERT:  20%|██        | 855/4272 [00:08<00:31, 107.33it/s]\u001b[A\nEncoding with PhoBERT:  20%|██        | 866/4272 [00:09<00:32, 103.41it/s]\u001b[A\nEncoding with PhoBERT:  21%|██        | 877/4272 [00:09<00:33, 101.40it/s]\u001b[A\nEncoding with PhoBERT:  21%|██        | 888/4272 [00:09<00:32, 103.58it/s]\u001b[A\nEncoding with PhoBERT:  21%|██        | 899/4272 [00:09<00:32, 104.28it/s]\u001b[A\nEncoding with PhoBERT:  21%|██▏       | 910/4272 [00:09<00:32, 105.00it/s]\u001b[A\nEncoding with PhoBERT:  22%|██▏       | 921/4272 [00:09<00:31, 105.05it/s]\u001b[A\nEncoding with PhoBERT:  22%|██▏       | 934/4272 [00:09<00:30, 109.70it/s]\u001b[A\nEncoding with PhoBERT:  22%|██▏       | 945/4272 [00:09<00:30, 109.74it/s]\u001b[A\nEncoding with PhoBERT:  22%|██▏       | 956/4272 [00:09<00:31, 106.72it/s]\u001b[A\nEncoding with PhoBERT:  23%|██▎       | 967/4272 [00:10<00:30, 106.65it/s]\u001b[A\nEncoding with PhoBERT:  23%|██▎       | 978/4272 [00:10<00:30, 106.89it/s]\u001b[A\nEncoding with PhoBERT:  23%|██▎       | 989/4272 [00:10<00:31, 105.56it/s]\u001b[A\nEncoding with PhoBERT:  23%|██▎       | 1000/4272 [00:10<00:32, 100.70it/s]\u001b[A\nEncoding with PhoBERT:  24%|██▎       | 1012/4272 [00:10<00:31, 103.38it/s]\u001b[A\nEncoding with PhoBERT:  24%|██▍       | 1023/4272 [00:10<00:30, 105.17it/s]\u001b[A\nEncoding with PhoBERT:  24%|██▍       | 1034/4272 [00:10<00:30, 104.80it/s]\u001b[A\nEncoding with PhoBERT:  24%|██▍       | 1046/4272 [00:10<00:30, 105.90it/s]\u001b[A\nEncoding with PhoBERT:  25%|██▍       | 1058/4272 [00:10<00:29, 107.17it/s]\u001b[A\nEncoding with PhoBERT:  25%|██▌       | 1070/4272 [00:11<00:29, 108.68it/s]\u001b[A\nEncoding with PhoBERT:  25%|██▌       | 1082/4272 [00:11<00:29, 109.05it/s]\u001b[A\nEncoding with PhoBERT:  26%|██▌       | 1093/4272 [00:11<00:29, 108.69it/s]\u001b[A\nEncoding with PhoBERT:  26%|██▌       | 1105/4272 [00:11<00:29, 109.13it/s]\u001b[A\nEncoding with PhoBERT:  26%|██▌       | 1116/4272 [00:11<00:29, 108.05it/s]\u001b[A\nEncoding with PhoBERT:  26%|██▋       | 1127/4272 [00:11<00:29, 106.47it/s]\u001b[A\nEncoding with PhoBERT:  27%|██▋       | 1139/4272 [00:11<00:28, 108.41it/s]\u001b[A\nEncoding with PhoBERT:  27%|██▋       | 1151/4272 [00:11<00:28, 110.53it/s]\u001b[A\nEncoding with PhoBERT:  27%|██▋       | 1163/4272 [00:11<00:27, 112.40it/s]\u001b[A\nEncoding with PhoBERT:  28%|██▊       | 1175/4272 [00:11<00:27, 110.96it/s]\u001b[A\nEncoding with PhoBERT:  28%|██▊       | 1188/4272 [00:12<00:27, 113.65it/s]\u001b[A\nEncoding with PhoBERT:  28%|██▊       | 1200/4272 [00:12<00:27, 111.92it/s]\u001b[A\nEncoding with PhoBERT:  28%|██▊       | 1212/4272 [00:12<00:27, 109.40it/s]\u001b[A\nEncoding with PhoBERT:  29%|██▊       | 1224/4272 [00:12<00:27, 110.00it/s]\u001b[A\nEncoding with PhoBERT:  29%|██▉       | 1236/4272 [00:12<00:28, 107.29it/s]\u001b[A\nEncoding with PhoBERT:  29%|██▉       | 1248/4272 [00:12<00:28, 107.88it/s]\u001b[A\nEncoding with PhoBERT:  29%|██▉       | 1259/4272 [00:12<00:27, 108.44it/s]\u001b[A\nEncoding with PhoBERT:  30%|██▉       | 1270/4272 [00:12<00:28, 105.82it/s]\u001b[A\nEncoding with PhoBERT:  30%|██▉       | 1281/4272 [00:12<00:28, 106.14it/s]\u001b[A\nEncoding with PhoBERT:  30%|███       | 1292/4272 [00:13<00:28, 105.84it/s]\u001b[A\nEncoding with PhoBERT:  31%|███       | 1304/4272 [00:13<00:27, 107.81it/s]\u001b[A\nEncoding with PhoBERT:  31%|███       | 1316/4272 [00:13<00:26, 109.78it/s]\u001b[A\nEncoding with PhoBERT:  31%|███       | 1328/4272 [00:13<00:26, 110.20it/s]\u001b[A\nEncoding with PhoBERT:  31%|███▏      | 1340/4272 [00:13<00:26, 111.36it/s]\u001b[A\nEncoding with PhoBERT:  32%|███▏      | 1352/4272 [00:13<00:27, 108.14it/s]\u001b[A\nEncoding with PhoBERT:  32%|███▏      | 1363/4272 [00:13<00:27, 106.94it/s]\u001b[A\nEncoding with PhoBERT:  32%|███▏      | 1374/4272 [00:13<00:26, 107.57it/s]\u001b[A\nEncoding with PhoBERT:  32%|███▏      | 1385/4272 [00:13<00:26, 107.51it/s]\u001b[A\nEncoding with PhoBERT:  33%|███▎      | 1397/4272 [00:14<00:26, 107.93it/s]\u001b[A\nEncoding with PhoBERT:  33%|███▎      | 1408/4272 [00:14<00:26, 106.64it/s]\u001b[A\nEncoding with PhoBERT:  33%|███▎      | 1419/4272 [00:14<00:26, 106.62it/s]\u001b[A\nEncoding with PhoBERT:  33%|███▎      | 1430/4272 [00:14<00:26, 107.52it/s]\u001b[A\nEncoding with PhoBERT:  34%|███▍      | 1443/4272 [00:14<00:25, 111.13it/s]\u001b[A\nEncoding with PhoBERT:  34%|███▍      | 1455/4272 [00:14<00:26, 108.15it/s]\u001b[A\nEncoding with PhoBERT:  34%|███▍      | 1466/4272 [00:14<00:25, 107.93it/s]\u001b[A\nEncoding with PhoBERT:  35%|███▍      | 1477/4272 [00:14<00:25, 108.30it/s]\u001b[A\nEncoding with PhoBERT:  35%|███▍      | 1488/4272 [00:14<00:26, 106.27it/s]\u001b[A\nEncoding with PhoBERT:  35%|███▌      | 1499/4272 [00:14<00:26, 105.38it/s]\u001b[A\nEncoding with PhoBERT:  35%|███▌      | 1511/4272 [00:15<00:25, 107.27it/s]\u001b[A\nEncoding with PhoBERT:  36%|███▌      | 1522/4272 [00:15<00:25, 107.04it/s]\u001b[A\nEncoding with PhoBERT:  36%|███▌      | 1533/4272 [00:15<00:25, 106.26it/s]\u001b[A\nEncoding with PhoBERT:  36%|███▌      | 1544/4272 [00:15<00:26, 104.14it/s]\u001b[A\nEncoding with PhoBERT:  36%|███▋      | 1555/4272 [00:15<00:26, 104.06it/s]\u001b[A\nEncoding with PhoBERT:  37%|███▋      | 1567/4272 [00:15<00:25, 106.86it/s]\u001b[A\nEncoding with PhoBERT:  37%|███▋      | 1578/4272 [00:15<00:25, 105.64it/s]\u001b[A\nEncoding with PhoBERT:  37%|███▋      | 1589/4272 [00:15<00:25, 104.37it/s]\u001b[A\nEncoding with PhoBERT:  37%|███▋      | 1600/4272 [00:15<00:25, 105.05it/s]\u001b[A\nEncoding with PhoBERT:  38%|███▊      | 1611/4272 [00:16<00:25, 106.16it/s]\u001b[A\nEncoding with PhoBERT:  38%|███▊      | 1622/4272 [00:16<00:25, 104.71it/s]\u001b[A\nEncoding with PhoBERT:  38%|███▊      | 1633/4272 [00:16<00:25, 105.05it/s]\u001b[A\nEncoding with PhoBERT:  38%|███▊      | 1644/4272 [00:16<00:25, 104.69it/s]\u001b[A\nEncoding with PhoBERT:  39%|███▊      | 1655/4272 [00:16<00:24, 104.87it/s]\u001b[A\nEncoding with PhoBERT:  39%|███▉      | 1666/4272 [00:16<00:25, 104.18it/s]\u001b[A\nEncoding with PhoBERT:  39%|███▉      | 1677/4272 [00:16<00:24, 105.42it/s]\u001b[A\nEncoding with PhoBERT:  40%|███▉      | 1688/4272 [00:16<00:25, 100.38it/s]\u001b[A\nEncoding with PhoBERT:  40%|███▉      | 1699/4272 [00:16<00:27, 94.82it/s] \u001b[A\nEncoding with PhoBERT:  40%|████      | 1709/4272 [00:17<00:26, 95.75it/s]\u001b[A\nEncoding with PhoBERT:  40%|████      | 1719/4272 [00:17<00:26, 96.84it/s]\u001b[A\nEncoding with PhoBERT:  40%|████      | 1729/4272 [00:17<00:26, 96.86it/s]\u001b[A\nEncoding with PhoBERT:  41%|████      | 1740/4272 [00:17<00:25, 98.83it/s]\u001b[A\nEncoding with PhoBERT:  41%|████      | 1751/4272 [00:17<00:25, 100.29it/s]\u001b[A\nEncoding with PhoBERT:  41%|████      | 1762/4272 [00:17<00:24, 100.72it/s]\u001b[A\nEncoding with PhoBERT:  42%|████▏     | 1773/4272 [00:17<00:24, 100.65it/s]\u001b[A\nEncoding with PhoBERT:  42%|████▏     | 1784/4272 [00:17<00:24, 102.49it/s]\u001b[A\nEncoding with PhoBERT:  42%|████▏     | 1795/4272 [00:17<00:23, 103.32it/s]\u001b[A\nEncoding with PhoBERT:  42%|████▏     | 1806/4272 [00:17<00:23, 104.75it/s]\u001b[A\nEncoding with PhoBERT:  43%|████▎     | 1817/4272 [00:18<00:23, 103.88it/s]\u001b[A\nEncoding with PhoBERT:  43%|████▎     | 1828/4272 [00:18<00:23, 104.51it/s]\u001b[A\nEncoding with PhoBERT:  43%|████▎     | 1839/4272 [00:18<00:23, 105.15it/s]\u001b[A\nEncoding with PhoBERT:  43%|████▎     | 1850/4272 [00:18<00:22, 106.01it/s]\u001b[A\nEncoding with PhoBERT:  44%|████▎     | 1862/4272 [00:18<00:22, 108.20it/s]\u001b[A\nEncoding with PhoBERT:  44%|████▍     | 1874/4272 [00:18<00:21, 109.77it/s]\u001b[A\nEncoding with PhoBERT:  44%|████▍     | 1886/4272 [00:18<00:21, 111.15it/s]\u001b[A\nEncoding with PhoBERT:  44%|████▍     | 1898/4272 [00:18<00:21, 108.69it/s]\u001b[A\nEncoding with PhoBERT:  45%|████▍     | 1909/4272 [00:18<00:22, 106.54it/s]\u001b[A\nEncoding with PhoBERT:  45%|████▍     | 1920/4272 [00:19<00:22, 104.07it/s]\u001b[A\nEncoding with PhoBERT:  45%|████▌     | 1931/4272 [00:19<00:23, 98.91it/s] \u001b[A\nEncoding with PhoBERT:  45%|████▌     | 1942/4272 [00:19<00:23, 100.94it/s]\u001b[A\nEncoding with PhoBERT:  46%|████▌     | 1953/4272 [00:19<00:22, 102.07it/s]\u001b[A\nEncoding with PhoBERT:  46%|████▌     | 1964/4272 [00:19<00:22, 101.66it/s]\u001b[A\nEncoding with PhoBERT:  46%|████▌     | 1975/4272 [00:19<00:22, 101.96it/s]\u001b[A\nEncoding with PhoBERT:  46%|████▋     | 1986/4272 [00:19<00:22, 103.72it/s]\u001b[A\nEncoding with PhoBERT:  47%|████▋     | 1997/4272 [00:19<00:22, 102.50it/s]\u001b[A\nEncoding with PhoBERT:  47%|████▋     | 2008/4272 [00:19<00:21, 104.10it/s]\u001b[A\nEncoding with PhoBERT:  47%|████▋     | 2019/4272 [00:19<00:21, 103.91it/s]\u001b[A\nEncoding with PhoBERT:  48%|████▊     | 2030/4272 [00:20<00:21, 104.11it/s]\u001b[A\nEncoding with PhoBERT:  48%|████▊     | 2041/4272 [00:20<00:21, 104.83it/s]\u001b[A\nEncoding with PhoBERT:  48%|████▊     | 2052/4272 [00:20<00:21, 105.68it/s]\u001b[A\nEncoding with PhoBERT:  48%|████▊     | 2063/4272 [00:20<00:20, 106.12it/s]\u001b[A\nEncoding with PhoBERT:  49%|████▊     | 2074/4272 [00:20<00:21, 101.37it/s]\u001b[A\nEncoding with PhoBERT:  49%|████▉     | 2085/4272 [00:20<00:21, 100.09it/s]\u001b[A\nEncoding with PhoBERT:  49%|████▉     | 2096/4272 [00:20<00:21, 99.94it/s] \u001b[A\nEncoding with PhoBERT:  49%|████▉     | 2107/4272 [00:20<00:21, 99.93it/s]\u001b[A\nEncoding with PhoBERT:  50%|████▉     | 2118/4272 [00:20<00:21, 102.30it/s]\u001b[A\nEncoding with PhoBERT:  50%|████▉     | 2129/4272 [00:21<00:20, 104.08it/s]\u001b[A\nEncoding with PhoBERT:  50%|█████     | 2140/4272 [00:21<00:20, 103.07it/s]\u001b[A\nEncoding with PhoBERT:  50%|█████     | 2151/4272 [00:21<00:20, 102.44it/s]\u001b[A\nEncoding with PhoBERT:  51%|█████     | 2162/4272 [00:21<00:20, 102.90it/s]\u001b[A\nEncoding with PhoBERT:  51%|█████     | 2173/4272 [00:21<00:20, 103.84it/s]\u001b[A\nEncoding with PhoBERT:  51%|█████     | 2185/4272 [00:21<00:19, 105.79it/s]\u001b[A\nEncoding with PhoBERT:  51%|█████▏    | 2197/4272 [00:21<00:19, 106.60it/s]\u001b[A\nEncoding with PhoBERT:  52%|█████▏    | 2208/4272 [00:21<00:19, 107.01it/s]\u001b[A\nEncoding with PhoBERT:  52%|█████▏    | 2219/4272 [00:21<00:19, 106.14it/s]\u001b[A\nEncoding with PhoBERT:  52%|█████▏    | 2230/4272 [00:22<00:19, 105.94it/s]\u001b[A\nEncoding with PhoBERT:  52%|█████▏    | 2241/4272 [00:22<00:19, 106.14it/s]\u001b[A\nEncoding with PhoBERT:  53%|█████▎    | 2252/4272 [00:22<00:18, 106.99it/s]\u001b[A\nEncoding with PhoBERT:  53%|█████▎    | 2264/4272 [00:22<00:18, 108.46it/s]\u001b[A\nEncoding with PhoBERT:  53%|█████▎    | 2275/4272 [00:22<00:18, 106.46it/s]\u001b[A\nEncoding with PhoBERT:  54%|█████▎    | 2286/4272 [00:22<00:18, 106.64it/s]\u001b[A\nEncoding with PhoBERT:  54%|█████▍    | 2298/4272 [00:22<00:18, 107.33it/s]\u001b[A\nEncoding with PhoBERT:  54%|█████▍    | 2309/4272 [00:22<00:18, 106.31it/s]\u001b[A\nEncoding with PhoBERT:  54%|█████▍    | 2320/4272 [00:22<00:18, 106.89it/s]\u001b[A\nEncoding with PhoBERT:  55%|█████▍    | 2332/4272 [00:22<00:17, 109.83it/s]\u001b[A\nEncoding with PhoBERT:  55%|█████▍    | 2343/4272 [00:23<00:17, 108.09it/s]\u001b[A\nEncoding with PhoBERT:  55%|█████▌    | 2354/4272 [00:23<00:17, 106.63it/s]\u001b[A\nEncoding with PhoBERT:  55%|█████▌    | 2365/4272 [00:23<00:18, 105.30it/s]\u001b[A\nEncoding with PhoBERT:  56%|█████▌    | 2376/4272 [00:23<00:18, 104.51it/s]\u001b[A\nEncoding with PhoBERT:  56%|█████▌    | 2388/4272 [00:23<00:17, 105.93it/s]\u001b[A\nEncoding with PhoBERT:  56%|█████▌    | 2399/4272 [00:23<00:17, 106.05it/s]\u001b[A\nEncoding with PhoBERT:  56%|█████▋    | 2410/4272 [00:23<00:17, 105.80it/s]\u001b[A\nEncoding with PhoBERT:  57%|█████▋    | 2421/4272 [00:23<00:17, 105.20it/s]\u001b[A\nEncoding with PhoBERT:  57%|█████▋    | 2432/4272 [00:23<00:17, 105.56it/s]\u001b[A\nEncoding with PhoBERT:  57%|█████▋    | 2443/4272 [00:24<00:17, 106.00it/s]\u001b[A\nEncoding with PhoBERT:  57%|█████▋    | 2454/4272 [00:24<00:17, 104.93it/s]\u001b[A\nEncoding with PhoBERT:  58%|█████▊    | 2466/4272 [00:24<00:16, 107.00it/s]\u001b[A\nEncoding with PhoBERT:  58%|█████▊    | 2477/4272 [00:24<00:16, 107.58it/s]\u001b[A\nEncoding with PhoBERT:  58%|█████▊    | 2488/4272 [00:24<00:16, 107.52it/s]\u001b[A\nEncoding with PhoBERT:  59%|█████▊    | 2500/4272 [00:24<00:16, 107.81it/s]\u001b[A\nEncoding with PhoBERT:  59%|█████▉    | 2511/4272 [00:24<00:16, 104.93it/s]\u001b[A\nEncoding with PhoBERT:  59%|█████▉    | 2522/4272 [00:24<00:16, 105.99it/s]\u001b[A\nEncoding with PhoBERT:  59%|█████▉    | 2533/4272 [00:24<00:16, 104.91it/s]\u001b[A\nEncoding with PhoBERT:  60%|█████▉    | 2545/4272 [00:24<00:16, 106.44it/s]\u001b[A\nEncoding with PhoBERT:  60%|█████▉    | 2557/4272 [00:25<00:15, 107.53it/s]\u001b[A\nEncoding with PhoBERT:  60%|██████    | 2568/4272 [00:25<00:15, 107.39it/s]\u001b[A\nEncoding with PhoBERT:  60%|██████    | 2579/4272 [00:25<00:15, 106.92it/s]\u001b[A\nEncoding with PhoBERT:  61%|██████    | 2590/4272 [00:25<00:15, 106.31it/s]\u001b[A\nEncoding with PhoBERT:  61%|██████    | 2601/4272 [00:25<00:15, 106.76it/s]\u001b[A\nEncoding with PhoBERT:  61%|██████    | 2612/4272 [00:25<00:16, 102.10it/s]\u001b[A\nEncoding with PhoBERT:  61%|██████▏   | 2623/4272 [00:25<00:16, 102.91it/s]\u001b[A\nEncoding with PhoBERT:  62%|██████▏   | 2634/4272 [00:25<00:15, 104.60it/s]\u001b[A\nEncoding with PhoBERT:  62%|██████▏   | 2645/4272 [00:25<00:15, 104.80it/s]\u001b[A\nEncoding with PhoBERT:  62%|██████▏   | 2657/4272 [00:26<00:15, 107.24it/s]\u001b[A\nEncoding with PhoBERT:  62%|██████▏   | 2668/4272 [00:26<00:15, 106.02it/s]\u001b[A\nEncoding with PhoBERT:  63%|██████▎   | 2680/4272 [00:26<00:14, 106.82it/s]\u001b[A\nEncoding with PhoBERT:  63%|██████▎   | 2691/4272 [00:26<00:15, 105.22it/s]\u001b[A\nEncoding with PhoBERT:  63%|██████▎   | 2702/4272 [00:26<00:14, 105.10it/s]\u001b[A\nEncoding with PhoBERT:  64%|██████▎   | 2713/4272 [00:26<00:14, 105.10it/s]\u001b[A\nEncoding with PhoBERT:  64%|██████▍   | 2724/4272 [00:26<00:15, 102.90it/s]\u001b[A\nEncoding with PhoBERT:  64%|██████▍   | 2735/4272 [00:26<00:14, 102.97it/s]\u001b[A\nEncoding with PhoBERT:  64%|██████▍   | 2746/4272 [00:26<00:14, 103.05it/s]\u001b[A\nEncoding with PhoBERT:  65%|██████▍   | 2757/4272 [00:26<00:14, 104.85it/s]\u001b[A\nEncoding with PhoBERT:  65%|██████▍   | 2768/4272 [00:27<00:14, 105.31it/s]\u001b[A\nEncoding with PhoBERT:  65%|██████▌   | 2779/4272 [00:27<00:14, 106.28it/s]\u001b[A\nEncoding with PhoBERT:  65%|██████▌   | 2790/4272 [00:27<00:14, 105.80it/s]\u001b[A\nEncoding with PhoBERT:  66%|██████▌   | 2801/4272 [00:27<00:14, 104.92it/s]\u001b[A\nEncoding with PhoBERT:  66%|██████▌   | 2813/4272 [00:27<00:13, 106.63it/s]\u001b[A\nEncoding with PhoBERT:  66%|██████▌   | 2824/4272 [00:27<00:13, 107.50it/s]\u001b[A\nEncoding with PhoBERT:  66%|██████▋   | 2836/4272 [00:27<00:13, 108.85it/s]\u001b[A\nEncoding with PhoBERT:  67%|██████▋   | 2847/4272 [00:27<00:13, 109.02it/s]\u001b[A\nEncoding with PhoBERT:  67%|██████▋   | 2858/4272 [00:27<00:13, 107.63it/s]\u001b[A\nEncoding with PhoBERT:  67%|██████▋   | 2869/4272 [00:28<00:13, 107.68it/s]\u001b[A\nEncoding with PhoBERT:  67%|██████▋   | 2881/4272 [00:28<00:12, 108.54it/s]\u001b[A\nEncoding with PhoBERT:  68%|██████▊   | 2892/4272 [00:28<00:12, 107.57it/s]\u001b[A\nEncoding with PhoBERT:  68%|██████▊   | 2903/4272 [00:28<00:12, 107.32it/s]\u001b[A\nEncoding with PhoBERT:  68%|██████▊   | 2914/4272 [00:28<00:12, 107.62it/s]\u001b[A\nEncoding with PhoBERT:  68%|██████▊   | 2925/4272 [00:28<00:12, 108.09it/s]\u001b[A\nEncoding with PhoBERT:  69%|██████▉   | 2937/4272 [00:28<00:12, 109.18it/s]\u001b[A\nEncoding with PhoBERT:  69%|██████▉   | 2948/4272 [00:28<00:12, 107.49it/s]\u001b[A\nEncoding with PhoBERT:  69%|██████▉   | 2959/4272 [00:28<00:12, 108.14it/s]\u001b[A\nEncoding with PhoBERT:  70%|██████▉   | 2970/4272 [00:28<00:12, 106.59it/s]\u001b[A\nEncoding with PhoBERT:  70%|██████▉   | 2981/4272 [00:29<00:12, 102.71it/s]\u001b[A\nEncoding with PhoBERT:  70%|███████   | 2992/4272 [00:29<00:12, 101.51it/s]\u001b[A\nEncoding with PhoBERT:  70%|███████   | 3003/4272 [00:29<00:12, 103.72it/s]\u001b[A\nEncoding with PhoBERT:  71%|███████   | 3014/4272 [00:29<00:12, 103.95it/s]\u001b[A\nEncoding with PhoBERT:  71%|███████   | 3025/4272 [00:29<00:11, 103.93it/s]\u001b[A\nEncoding with PhoBERT:  71%|███████   | 3036/4272 [00:29<00:11, 105.35it/s]\u001b[A\nEncoding with PhoBERT:  71%|███████▏  | 3047/4272 [00:29<00:11, 106.39it/s]\u001b[A\nEncoding with PhoBERT:  72%|███████▏  | 3058/4272 [00:29<00:11, 106.17it/s]\u001b[A\nEncoding with PhoBERT:  72%|███████▏  | 3069/4272 [00:29<00:11, 103.86it/s]\u001b[A\nEncoding with PhoBERT:  72%|███████▏  | 3081/4272 [00:30<00:11, 105.56it/s]\u001b[A\nEncoding with PhoBERT:  72%|███████▏  | 3093/4272 [00:30<00:10, 107.78it/s]\u001b[A\nEncoding with PhoBERT:  73%|███████▎  | 3104/4272 [00:30<00:10, 107.39it/s]\u001b[A\nEncoding with PhoBERT:  73%|███████▎  | 3115/4272 [00:30<00:11, 104.98it/s]\u001b[A\nEncoding with PhoBERT:  73%|███████▎  | 3126/4272 [00:30<00:11, 103.37it/s]\u001b[A\nEncoding with PhoBERT:  73%|███████▎  | 3137/4272 [00:30<00:10, 104.83it/s]\u001b[A\nEncoding with PhoBERT:  74%|███████▎  | 3148/4272 [00:30<00:10, 102.61it/s]\u001b[A\nEncoding with PhoBERT:  74%|███████▍  | 3159/4272 [00:30<00:11, 95.57it/s] \u001b[A\nEncoding with PhoBERT:  74%|███████▍  | 3169/4272 [00:30<00:11, 92.30it/s]\u001b[A\nEncoding with PhoBERT:  74%|███████▍  | 3179/4272 [00:31<00:11, 91.27it/s]\u001b[A\nEncoding with PhoBERT:  75%|███████▍  | 3189/4272 [00:31<00:12, 89.59it/s]\u001b[A\nEncoding with PhoBERT:  75%|███████▍  | 3199/4272 [00:31<00:11, 90.55it/s]\u001b[A\nEncoding with PhoBERT:  75%|███████▌  | 3209/4272 [00:31<00:11, 89.41it/s]\u001b[A\nEncoding with PhoBERT:  75%|███████▌  | 3218/4272 [00:31<00:12, 87.13it/s]\u001b[A\nEncoding with PhoBERT:  76%|███████▌  | 3228/4272 [00:31<00:11, 89.62it/s]\u001b[A\nEncoding with PhoBERT:  76%|███████▌  | 3237/4272 [00:31<00:11, 88.95it/s]\u001b[A\nEncoding with PhoBERT:  76%|███████▌  | 3246/4272 [00:31<00:11, 88.97it/s]\u001b[A\nEncoding with PhoBERT:  76%|███████▌  | 3257/4272 [00:31<00:10, 94.38it/s]\u001b[A\nEncoding with PhoBERT:  76%|███████▋  | 3268/4272 [00:32<00:10, 98.02it/s]\u001b[A\nEncoding with PhoBERT:  77%|███████▋  | 3279/4272 [00:32<00:10, 99.11it/s]\u001b[A\nEncoding with PhoBERT:  77%|███████▋  | 3291/4272 [00:32<00:09, 102.88it/s]\u001b[A\nEncoding with PhoBERT:  77%|███████▋  | 3302/4272 [00:32<00:09, 101.92it/s]\u001b[A\nEncoding with PhoBERT:  78%|███████▊  | 3314/4272 [00:32<00:09, 103.98it/s]\u001b[A\nEncoding with PhoBERT:  78%|███████▊  | 3326/4272 [00:32<00:09, 105.03it/s]\u001b[A\nEncoding with PhoBERT:  78%|███████▊  | 3337/4272 [00:32<00:08, 105.64it/s]\u001b[A\nEncoding with PhoBERT:  78%|███████▊  | 3348/4272 [00:32<00:08, 105.30it/s]\u001b[A\nEncoding with PhoBERT:  79%|███████▊  | 3359/4272 [00:32<00:08, 106.04it/s]\u001b[A\nEncoding with PhoBERT:  79%|███████▉  | 3371/4272 [00:32<00:08, 107.81it/s]\u001b[A\nEncoding with PhoBERT:  79%|███████▉  | 3383/4272 [00:33<00:08, 108.79it/s]\u001b[A\nEncoding with PhoBERT:  79%|███████▉  | 3394/4272 [00:33<00:08, 106.54it/s]\u001b[A\nEncoding with PhoBERT:  80%|███████▉  | 3405/4272 [00:33<00:08, 106.22it/s]\u001b[A\nEncoding with PhoBERT:  80%|███████▉  | 3416/4272 [00:33<00:08, 106.64it/s]\u001b[A\nEncoding with PhoBERT:  80%|████████  | 3427/4272 [00:33<00:07, 106.23it/s]\u001b[A\nEncoding with PhoBERT:  80%|████████  | 3438/4272 [00:33<00:07, 106.56it/s]\u001b[A\nEncoding with PhoBERT:  81%|████████  | 3449/4272 [00:33<00:07, 107.41it/s]\u001b[A\nEncoding with PhoBERT:  81%|████████  | 3461/4272 [00:33<00:07, 109.12it/s]\u001b[A\nEncoding with PhoBERT:  81%|████████▏ | 3472/4272 [00:33<00:07, 106.28it/s]\u001b[A\nEncoding with PhoBERT:  82%|████████▏ | 3483/4272 [00:34<00:07, 104.69it/s]\u001b[A\nEncoding with PhoBERT:  82%|████████▏ | 3494/4272 [00:34<00:07, 103.87it/s]\u001b[A\nEncoding with PhoBERT:  82%|████████▏ | 3505/4272 [00:34<00:07, 104.63it/s]\u001b[A\nEncoding with PhoBERT:  82%|████████▏ | 3516/4272 [00:34<00:07, 105.32it/s]\u001b[A\nEncoding with PhoBERT:  83%|████████▎ | 3527/4272 [00:34<00:07, 105.04it/s]\u001b[A\nEncoding with PhoBERT:  83%|████████▎ | 3538/4272 [00:34<00:07, 104.60it/s]\u001b[A\nEncoding with PhoBERT:  83%|████████▎ | 3549/4272 [00:34<00:06, 105.68it/s]\u001b[A\nEncoding with PhoBERT:  83%|████████▎ | 3560/4272 [00:34<00:06, 104.16it/s]\u001b[A\nEncoding with PhoBERT:  84%|████████▎ | 3571/4272 [00:34<00:06, 104.47it/s]\u001b[A\nEncoding with PhoBERT:  84%|████████▍ | 3583/4272 [00:34<00:06, 107.34it/s]\u001b[A\nEncoding with PhoBERT:  84%|████████▍ | 3595/4272 [00:35<00:06, 108.46it/s]\u001b[A\nEncoding with PhoBERT:  84%|████████▍ | 3606/4272 [00:35<00:06, 106.84it/s]\u001b[A\nEncoding with PhoBERT:  85%|████████▍ | 3618/4272 [00:35<00:05, 109.41it/s]\u001b[A\nEncoding with PhoBERT:  85%|████████▍ | 3630/4272 [00:35<00:05, 110.73it/s]\u001b[A\nEncoding with PhoBERT:  85%|████████▌ | 3642/4272 [00:35<00:05, 108.87it/s]\u001b[A\nEncoding with PhoBERT:  86%|████████▌ | 3653/4272 [00:35<00:05, 106.55it/s]\u001b[A\nEncoding with PhoBERT:  86%|████████▌ | 3665/4272 [00:35<00:05, 107.77it/s]\u001b[A\nEncoding with PhoBERT:  86%|████████▌ | 3676/4272 [00:35<00:05, 107.14it/s]\u001b[A\nEncoding with PhoBERT:  86%|████████▋ | 3687/4272 [00:35<00:05, 107.90it/s]\u001b[A\nEncoding with PhoBERT:  87%|████████▋ | 3698/4272 [00:36<00:05, 108.09it/s]\u001b[A\nEncoding with PhoBERT:  87%|████████▋ | 3710/4272 [00:36<00:05, 109.52it/s]\u001b[A\nEncoding with PhoBERT:  87%|████████▋ | 3721/4272 [00:36<00:05, 108.04it/s]\u001b[A\nEncoding with PhoBERT:  87%|████████▋ | 3732/4272 [00:36<00:05, 106.16it/s]\u001b[A\nEncoding with PhoBERT:  88%|████████▊ | 3743/4272 [00:36<00:05, 105.63it/s]\u001b[A\nEncoding with PhoBERT:  88%|████████▊ | 3754/4272 [00:36<00:04, 106.15it/s]\u001b[A\nEncoding with PhoBERT:  88%|████████▊ | 3765/4272 [00:36<00:04, 106.69it/s]\u001b[A\nEncoding with PhoBERT:  88%|████████▊ | 3776/4272 [00:36<00:04, 105.11it/s]\u001b[A\nEncoding with PhoBERT:  89%|████████▊ | 3788/4272 [00:36<00:04, 107.98it/s]\u001b[A\nEncoding with PhoBERT:  89%|████████▉ | 3800/4272 [00:36<00:04, 109.44it/s]\u001b[A\nEncoding with PhoBERT:  89%|████████▉ | 3811/4272 [00:37<00:04, 107.93it/s]\u001b[A\nEncoding with PhoBERT:  89%|████████▉ | 3822/4272 [00:37<00:04, 106.52it/s]\u001b[A\nEncoding with PhoBERT:  90%|████████▉ | 3833/4272 [00:37<00:04, 105.21it/s]\u001b[A\nEncoding with PhoBERT:  90%|████████▉ | 3844/4272 [00:37<00:04, 104.82it/s]\u001b[A\nEncoding with PhoBERT:  90%|█████████ | 3855/4272 [00:37<00:04, 104.19it/s]\u001b[A\nEncoding with PhoBERT:  90%|█████████ | 3866/4272 [00:37<00:03, 103.41it/s]\u001b[A\nEncoding with PhoBERT:  91%|█████████ | 3877/4272 [00:37<00:03, 103.41it/s]\u001b[A\nEncoding with PhoBERT:  91%|█████████ | 3888/4272 [00:37<00:03, 104.48it/s]\u001b[A\nEncoding with PhoBERT:  91%|█████████▏| 3900/4272 [00:37<00:03, 107.23it/s]\u001b[A\nEncoding with PhoBERT:  92%|█████████▏| 3911/4272 [00:38<00:03, 105.47it/s]\u001b[A\nEncoding with PhoBERT:  92%|█████████▏| 3922/4272 [00:38<00:03, 105.56it/s]\u001b[A\nEncoding with PhoBERT:  92%|█████████▏| 3933/4272 [00:38<00:03, 105.15it/s]\u001b[A\nEncoding with PhoBERT:  92%|█████████▏| 3944/4272 [00:38<00:03, 103.72it/s]\u001b[A\nEncoding with PhoBERT:  93%|█████████▎| 3955/4272 [00:38<00:03, 105.03it/s]\u001b[A\nEncoding with PhoBERT:  93%|█████████▎| 3966/4272 [00:38<00:02, 105.68it/s]\u001b[A\nEncoding with PhoBERT:  93%|█████████▎| 3978/4272 [00:38<00:02, 107.78it/s]\u001b[A\nEncoding with PhoBERT:  93%|█████████▎| 3990/4272 [00:38<00:02, 109.31it/s]\u001b[A\nEncoding with PhoBERT:  94%|█████████▎| 4001/4272 [00:38<00:02, 106.95it/s]\u001b[A\nEncoding with PhoBERT:  94%|█████████▍| 4012/4272 [00:39<00:02, 107.02it/s]\u001b[A\nEncoding with PhoBERT:  94%|█████████▍| 4023/4272 [00:39<00:02, 101.20it/s]\u001b[A\nEncoding with PhoBERT:  94%|█████████▍| 4034/4272 [00:39<00:02, 103.28it/s]\u001b[A\nEncoding with PhoBERT:  95%|█████████▍| 4046/4272 [00:39<00:02, 105.50it/s]\u001b[A\nEncoding with PhoBERT:  95%|█████████▍| 4057/4272 [00:39<00:02, 105.67it/s]\u001b[A\nEncoding with PhoBERT:  95%|█████████▌| 4068/4272 [00:39<00:01, 104.85it/s]\u001b[A\nEncoding with PhoBERT:  96%|█████████▌| 4080/4272 [00:39<00:01, 108.38it/s]\u001b[A\nEncoding with PhoBERT:  96%|█████████▌| 4091/4272 [00:39<00:01, 108.03it/s]\u001b[A\nEncoding with PhoBERT:  96%|█████████▌| 4103/4272 [00:39<00:01, 109.89it/s]\u001b[A\nEncoding with PhoBERT:  96%|█████████▋| 4114/4272 [00:39<00:01, 108.67it/s]\u001b[A\nEncoding with PhoBERT:  97%|█████████▋| 4125/4272 [00:40<00:01, 108.58it/s]\u001b[A\nEncoding with PhoBERT:  97%|█████████▋| 4136/4272 [00:40<00:01, 106.06it/s]\u001b[A\nEncoding with PhoBERT:  97%|█████████▋| 4149/4272 [00:40<00:01, 111.11it/s]\u001b[A\nEncoding with PhoBERT:  97%|█████████▋| 4161/4272 [00:40<00:01, 109.18it/s]\u001b[A\nEncoding with PhoBERT:  98%|█████████▊| 4172/4272 [00:40<00:00, 107.41it/s]\u001b[A\nEncoding with PhoBERT:  98%|█████████▊| 4183/4272 [00:40<00:00, 105.44it/s]\u001b[A\nEncoding with PhoBERT:  98%|█████████▊| 4194/4272 [00:40<00:00, 105.52it/s]\u001b[A\nEncoding with PhoBERT:  98%|█████████▊| 4205/4272 [00:40<00:00, 104.75it/s]\u001b[A\nEncoding with PhoBERT:  99%|█████████▊| 4216/4272 [00:40<00:00, 104.17it/s]\u001b[A\nEncoding with PhoBERT:  99%|█████████▉| 4227/4272 [00:41<00:00, 105.02it/s]\u001b[A\nEncoding with PhoBERT:  99%|█████████▉| 4238/4272 [00:41<00:00, 104.84it/s]\u001b[A\nEncoding with PhoBERT:  99%|█████████▉| 4249/4272 [00:41<00:00, 101.48it/s]\u001b[A\nEncoding with PhoBERT: 100%|█████████▉| 4260/4272 [00:41<00:00, 100.33it/s]\u001b[A\nEncoding with PhoBERT: 100%|██████████| 4272/4272 [00:41<00:00, 102.98it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Embedding shape: (4272, 768)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Lưu trữ vào FAISS"],"metadata":{"id":"TfHMmJSvoX4L"}},{"cell_type":"code","source":["import faiss\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from datetime import datetime, timedelta\n","from tqdm import tqdm\n","\n","# Normalize embedding trước khi add\n","faiss.normalize_L2(embeddings)\n","\n","# Chuẩn bị dimension\n","dimension = embeddings.shape[1]\n","\n","# Khởi tạo FAISS Index\n","index = faiss.IndexFlatIP(dimension)   # Dùng Inner Product thay vì L2 (vì đã normalize rồi)\n","\n","# Add vào FAISS index\n","index.add(embeddings)\n","\n","print(\"FAISS index có số vector:\", index.ntotal)\n","\n","# Gán mapping index vào df_chunks\n","df_chunks['embedding_index'] = list(range(len(df_chunks)))\n","faiss.write_index(index, \"phobert_index.faiss\")\n","\n","# IF-IDF\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(df_chunks['text'])\n"],"metadata":{"id":"_i1PtrfwjWmR","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:28.303783Z","iopub.execute_input":"2025-05-09T03:19:28.303971Z","iopub.status.idle":"2025-05-09T03:19:28.598222Z","shell.execute_reply.started":"2025-05-09T03:19:28.303955Z","shell.execute_reply":"2025-05-09T03:19:28.597661Z"},"outputId":"56232de4-9fd0-446f-84a4-0f27c99be4fe"},"outputs":[{"name":"stdout","text":"FAISS index có số vector: 4272\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Metadata Filtering"],"metadata":{"id":"VKS7ah1OOEen"}},{"cell_type":"code","source":["def hybrid_search_with_metadata(query, top_k=10, alpha=0.5, ticker=None, source=None, date_range=None):\n","    \"\"\"\n","    Kết hợp FAISS + TF-IDF search, sau đó lọc kết quả bằng metadata.\n","    \"\"\"\n","    # Bước 1: Semantic + Lexical search (FAISS + TF-IDF)\n","    query_vec = encode_phobert(query)  # (1, dim)\n","    faiss.normalize_L2(query_vec)\n","    D, I = index.search(query_vec, top_k * 50)  # tìm nhiều hơn để lọc sau\n","\n","    tfidf_query = vectorizer.transform([query])\n","    bm25_scores = cosine_similarity(tfidf_query, tfidf_matrix)[0]\n","\n","    combined_scores = {}\n","    for i in I[0]:\n","        score_faiss = D[0][np.where(I[0] == i)[0][0]]\n","        score_tfidf = bm25_scores[i]\n","        combined_scores[i] = alpha * score_faiss + (1 - alpha) * score_tfidf\n","\n","    sorted_candidates = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Bước 2: Metadata filtering\n","    filtered = []\n","    seen_indices = set()\n","    for i, _ in sorted_candidates:\n","        row = df_chunks.iloc[i]\n","        if ticker and row['ticker'].lower() != ticker.lower():\n","            continue\n","        if source and row['source'].lower() != source.lower():\n","            continue\n","        if date_range:\n","            rdate = pd.to_datetime(row[\"date\"], errors=\"coerce\")\n","            ldate = pd.to_datetime(row[\"record_date\"], errors=\"coerce\")\n","            if pd.isna(rdate) or (not pd.isna(ldate) and (date_range[0] > rdate or date_range[1] < ldate)):\n","                continue\n","        filtered.append(row)\n","        seen_indices.add(i)\n","        if len(filtered) >= top_k:\n","            break\n","\n","    # Bước 3: Fallback nếu không đủ\n","    if len(filtered) < top_k:\n","        for i, _ in sorted_candidates:\n","            if i not in seen_indices:\n","                filtered.append(df_chunks.iloc[i])\n","                seen_indices.add(i)\n","            if len(filtered) >= top_k:\n","                break\n","\n","    return filtered\n"],"metadata":{"id":"-Zje_aIVuDdC","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:28.598986Z","iopub.execute_input":"2025-05-09T03:19:28.599232Z","iopub.status.idle":"2025-05-09T03:19:28.607278Z","shell.execute_reply.started":"2025-05-09T03:19:28.599207Z","shell.execute_reply":"2025-05-09T03:19:28.606731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Metadata Extraction from query"],"metadata":{"id":"4zJmEsr7UMpX"}},{"cell_type":"code","source":["def format_metadata_prompt_qwen_full(query):\n","    return f\"\"\"<|im_start|>system\n","Bạn là một hệ thống trích xuất metadata tài chính.\n","<|im_end|>\n","<|im_start|>user\n","Nhiệm vụ của bạn là đọc câu hỏi và xuất ra metadata ở dạng JSON với các trường sau:\n","\n","- \"ticker\": mã cổ phiếu trong câu hỏi (ví dụ: FPT, CMG). Nếu không rõ thì null.\n","- \"source\": nguồn nếu có (ví dụ: cafef, internal, shareholder). Nếu không có, để null.\n","- \"start_date\": ngày bắt đầu truy vấn, định dạng YYYY-MM-DD\n","- \"end_date\": ngày kết thúc truy vấn, định dạng YYYY-MM-DD\n","\n","QUY TẮC:\n","- Nếu câu hỏi đề cập đến \"quý\", hãy map sang mốc thời gian:\n","  - \"quý 1 năm 2025\" → \"start_date\": \"2025-01-01\", \"end_date\": \"2025-03-31\"\n","  - \"quý 2 năm 2025\" → \"start_date\": \"2025-04-01\", \"end_date\": \"2025-06-30\"\n","  - \"quý 3 năm 2025\" → \"start_date\": \"2025-07-01\", \"end_date\": \"2025-09-30\"\n","  - \"quý 4 năm 2025\" → \"start_date\": \"2025-10-01\", \"end_date\": \"2025-12-31\"\n","- Nếu câu hỏi chỉ đề cập đến \"năm 2025\" → start = \"2025-01-01\", end = \"2025-12-31\"\n","- Nếu nói \"năm ngoái\" → lấy năm hiện tại là 2024 → map thành 2023\n","- Nếu nói \"gần đây\", \"mới đây\", \"thời gian gần đây\" → chọn 3 tháng gần nhất tính từ hôm nay\n","- Nếu không có thông tin thời gian → start_date và end_date = null\n","\n","Câu hỏi: \"{query}\"\n","\n","Kết quả JSON:\n","<|im_end|>\n","<|im_start|>assistant\n","\"\"\"\n"],"metadata":{"id":"OFZH872DWXtL","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:28.608115Z","iopub.execute_input":"2025-05-09T03:19:28.608334Z","iopub.status.idle":"2025-05-09T03:19:28.625881Z","shell.execute_reply.started":"2025-05-09T03:19:28.608312Z","shell.execute_reply":"2025-05-09T03:19:28.625234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def parse_llm_json(text):\n","    try:\n","        start = text.find(\"{\")\n","        end = text.rfind(\"}\") + 1\n","        return json.loads(text[start:end])\n","    except Exception as e:\n","        print(\"❌ Lỗi parse JSON:\", e)\n","        return {\n","            \"ticker\": None,\n","            \"start_date\": None,\n","            \"end_date\": None,\n","            \"source\": None\n","        }\n"],"metadata":{"id":"y_2FHgZ6W75P","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:28.626614Z","iopub.execute_input":"2025-05-09T03:19:28.626890Z","iopub.status.idle":"2025-05-09T03:19:28.644970Z","shell.execute_reply.started":"2025-05-09T03:19:28.626870Z","shell.execute_reply":"2025-05-09T03:19:28.644191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install hf_xet"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:28.647309Z","iopub.execute_input":"2025-05-09T03:19:28.647726Z","iopub.status.idle":"2025-05-09T03:19:33.550374Z","shell.execute_reply.started":"2025-05-09T03:19:28.647709Z","shell.execute_reply":"2025-05-09T03:19:33.549586Z"},"id":"ZwpmV7TWVLLg","outputId":"8ad3da4a-b06f-44f2-aae2-fdf1e220e92a"},"outputs":[{"name":"stdout","text":"Collecting hf_xet\n  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\nDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hf_xet\nSuccessfully installed hf_xet-1.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:33.551329Z","iopub.execute_input":"2025-05-09T03:19:33.551602Z","iopub.status.idle":"2025-05-09T03:19:45.674536Z","shell.execute_reply.started":"2025-05-09T03:19:33.551576Z","shell.execute_reply":"2025-05-09T03:19:45.673685Z"},"id":"E6Mj7iLzVLLg"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel, PeftConfig\n","\n","# Đường dẫn tới model trên Hugging Face Hub\n","model_name = \"KKcom0028/ragftqwen2\"\n","\n","# Load base model (Qwen2.5-7B-Instruct) từ Hugging Face\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"Qwen/Qwen2.5-7B-Instruct\",\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"  # Sử dụng GPU nếu có\n",")\n","\n","# Load model đã fine-tune với LoRA/QLoRA\n","model_llm = PeftModel.from_pretrained(\n","    base_model,\n","    model_name,\n","    adapter_name=\"default\"\n",")\n","\n","# Load tokenizer\n","tokenizer_llm = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n","\n","\n","\n","# Hàm trích metadata\n","def extract_metadata_with_qwen(question):\n","    prompt = format_metadata_prompt_qwen_full(question)  # giữ nguyên nếu prompt dùng được với llama\n","    inputs = tokenizer_llm(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n","\n","    with torch.no_grad():\n","        outputs = model_llm.generate(\n","            **inputs,\n","            max_new_tokens=100,\n","            use_cache=True\n","        )\n","\n","    output_text = tokenizer_llm.decode(outputs[0], skip_special_tokens=True)\n","    return output_text\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:19:48.864976Z","iopub.execute_input":"2025-05-09T03:19:48.865310Z","iopub.status.idle":"2025-05-09T03:21:16.870582Z","shell.execute_reply.started":"2025-05-09T03:19:48.865275Z","shell.execute_reply":"2025-05-09T03:21:16.869794Z"},"colab":{"referenced_widgets":["27f4a977524049f8ad909e1c035e79ec","e209629a3baf4bf4aa6f5b91982b7fd8","3f0669dfb3aa41cda21882ec2b79eab7","edb46b91747e4776b6c9f15289fd5f4a","ab163b99ca594cff94a517bb5974e72f","6dcf48f333a54f4486c1ea276fdd6ac0","61a7696f7e664494bea105e32922df21","04be03ee134c4636adfb30564016c0f4","0ffca4191ce44419a5c74bb657dc94d9","923bc702752b4724a419272835d748c8","847346099ee646008d6f3a5f3b6450b1","92eac6c68ef0496f97a57f6d97fe5f08","e1f84bb8343d468aa60059bbfd2d88e0","beefd5a4ef37473590e16c8e2a8f1c34","31fd52d7354f47a08754c36061ce70f1"]},"id":"l0AINl3GVLLg","outputId":"a53fb7d4-d3bf-4b1a-fe1a-ce97af95a8be"},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27f4a977524049f8ad909e1c035e79ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e209629a3baf4bf4aa6f5b91982b7fd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0669dfb3aa41cda21882ec2b79eab7"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb46b91747e4776b6c9f15289fd5f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab163b99ca594cff94a517bb5974e72f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dcf48f333a54f4486c1ea276fdd6ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a7696f7e664494bea105e32922df21"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04be03ee134c4636adfb30564016c0f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ffca4191ce44419a5c74bb657dc94d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/815 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923bc702752b4724a419272835d748c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/162M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"847346099ee646008d6f3a5f3b6450b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eac6c68ef0496f97a57f6d97fe5f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f84bb8343d468aa60059bbfd2d88e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beefd5a4ef37473590e16c8e2a8f1c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fd52d7354f47a08754c36061ce70f1"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["def search_with_qwen_metadata(query, top_k=10):\n","    metadata_raw = extract_metadata_with_qwen(query)\n","    metadata = parse_llm_json(metadata_raw)\n","\n","    ticker = metadata.get(\"ticker\") or None\n","    source = metadata.get(\"source\") or None\n","\n","    date_range = None\n","\n","    if metadata.get(\"start_date\") and metadata.get(\"end_date\"):\n","        date_range = (\n","            pd.to_datetime(metadata[\"start_date\"]),\n","            pd.to_datetime(metadata[\"end_date\"])\n","        )\n","    return hybrid_search_with_metadata(query, top_k=top_k, ticker=ticker, source=source, date_range=date_range)\n"],"metadata":{"id":"sQxN4qt5WYhH","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:21:16.877455Z","iopub.execute_input":"2025-05-09T03:21:16.878091Z","iopub.status.idle":"2025-05-09T03:21:16.898303Z","shell.execute_reply.started":"2025-05-09T03:21:16.878064Z","shell.execute_reply":"2025-05-09T03:21:16.897570Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Query Transformation"],"metadata":{"id":"QoLksub1odNa"}},{"cell_type":"code","source":["# prompt: lấy kết quả từ truy vấn retrieve và chuyển chúng thành một chuỗi văn bản có thể hiển thị\n","\n","def retrieve_and_format(query, top_k=10):\n","    results = search_with_qwen_metadata(query, top_k)\n","    formatted_results = \"\"\n","    for i, result in enumerate(results):\n","        formatted_results += f\"Thông tin {i+1}:\\n{result}\\n\"\n","    return formatted_results"],"metadata":{"id":"7Q-b9CsFKyoU","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:21:16.899102Z","iopub.execute_input":"2025-05-09T03:21:16.899362Z","iopub.status.idle":"2025-05-09T03:21:16.913400Z","shell.execute_reply.started":"2025-05-09T03:21:16.899338Z","shell.execute_reply":"2025-05-09T03:21:16.912630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["query = \"Lợi nhuận FPT có tăng vào quý 1 năm 2025 không?\"\n","source_information = retrieve_and_format(query, 10)\n","print(\"Phản hồi từ hệ thống RAG:\")\n","print(source_information)"],"metadata":{"id":"5V8v6c04nV-5","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:21:16.914257Z","iopub.execute_input":"2025-05-09T03:21:16.914528Z","iopub.status.idle":"2025-05-09T03:22:34.330536Z","shell.execute_reply.started":"2025-05-09T03:21:16.914509Z","shell.execute_reply":"2025-05-09T03:22:34.329865Z"},"outputId":"e2c6a62b-9fa7-4b2d-84a8-8b4941b398f0"},"outputs":[{"name":"stderr","text":"Encoding with PhoBERT: 100%|██████████| 43/43 [00:00<00:00, 131.40it/s]","output_type":"stream"},{"name":"stdout","text":"Phản hồi từ hệ thống RAG:\nThông tin 1:\ntext               Loại giao dịch: GD CĐ lớn. Người thực hiện: Ge...\nticker                                                           CMG\nrecord_date                                      2024-03-26 00:00:00\ndate                                             2024-03-26 00:00:00\nsource                                                      internal\nembedding_index                                                 4239\nName: 4239, dtype: object\nThông tin 2:\ntext               Loại giao dịch: GD CĐ lớn. Người thực hiện: Ge...\nticker                                                           CMG\nrecord_date                                      2024-03-05 00:00:00\ndate                                             2024-03-05 00:00:00\nsource                                                      internal\nembedding_index                                                 4240\nName: 4240, dtype: object\nThông tin 3:\ntext                                 .\nticker                             FPT\nrecord_date        2024-05-09 00:00:00\ndate               2024-05-09 00:00:00\nsource                           cafef\nembedding_index                   3428\nName: 3428, dtype: object\nThông tin 4:\ntext                                 .\nticker                             FPT\nrecord_date        2024-06-18 00:00:00\ndate               2024-06-18 00:00:00\nsource                           cafef\nembedding_index                   2883\nName: 2883, dtype: object\nThông tin 5:\ntext                                 .\nticker                             FPT\nrecord_date        2024-06-21 00:00:00\ndate               2024-06-21 00:00:00\nsource                           cafef\nembedding_index                   2849\nName: 2849, dtype: object\nThông tin 6:\ntext                                 .\nticker                             FPT\nrecord_date        2024-08-20 00:00:00\ndate               2024-08-20 00:00:00\nsource                           cafef\nembedding_index                   2166\nName: 2166, dtype: object\nThông tin 7:\ntext                                 .\nticker                             FPT\nrecord_date        2024-08-28 00:00:00\ndate               2024-08-28 00:00:00\nsource                           cafef\nembedding_index                   2078\nName: 2078, dtype: object\nThông tin 8:\ntext               . Tỷ lệ sau giao dịch: 8.97%.\nticker                                       CMG\nrecord_date                  2023-08-29 00:00:00\ndate                         2023-08-29 00:00:00\nsource                                  internal\nembedding_index                             4253\nName: 4253, dtype: object\nThông tin 9:\ntext               . Số lượng sau giao dịch: 20,836,081. Tỷ lệ sa...\nticker                                                           FPT\nrecord_date                                      2023-07-24 00:00:00\ndate                                             2023-07-24 00:00:00\nsource                                                      internal\nembedding_index                                                 4189\nName: 4189, dtype: object\nThông tin 10:\ntext               . Số lượng sau giao dịch: 826,220. Tỷ lệ sau g...\nticker                                                           FPT\nrecord_date                                      2023-10-10 00:00:00\ndate                                             2023-10-10 00:00:00\nsource                                                      internal\nembedding_index                                                 4179\nName: 4179, dtype: object\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Triển khai"],"metadata":{"id":"bVULZWC-fVGS"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"kG7rllnv27-a","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:22:34.336564Z","iopub.execute_input":"2025-05-09T03:22:34.336819Z","iopub.status.idle":"2025-05-09T03:22:37.607362Z","shell.execute_reply.started":"2025-05-09T03:22:34.336794Z","shell.execute_reply":"2025-05-09T03:22:37.606243Z"},"outputId":"313ba75d-3603-4d2f-84e6-c06312ff0ebc"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"id":"KKfRyAjCK5zx","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:22:37.608645Z","iopub.execute_input":"2025-05-09T03:22:37.609199Z","iopub.status.idle":"2025-05-09T03:22:40.668900Z","shell.execute_reply.started":"2025-05-09T03:22:37.609173Z","shell.execute_reply":"2025-05-09T03:22:40.667883Z"},"outputId":"17b7b1d0-a0d5-4252-91a2-5a33f648118e"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["!pip install transformers_stream_generator"],"metadata":{"id":"UrPYHGmnLK_P","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:22:40.670183Z","iopub.execute_input":"2025-05-09T03:22:40.670498Z","iopub.status.idle":"2025-05-09T03:22:45.720309Z","shell.execute_reply.started":"2025-05-09T03:22:40.670466Z","shell.execute_reply":"2025-05-09T03:22:45.719384Z"},"outputId":"5fea7faf-ec6a-499b-9881-60a177f8f290"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting transformers_stream_generator\n  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.11/dist-packages (from transformers_stream_generator) (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.26.1->transformers_stream_generator) (2024.2.0)\nBuilding wheels for collected packages: transformers_stream_generator\n  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12425 sha256=77f3f81aa28504d917c81cc5ca80fdf502564b1cbe7cc908b1d6e431925a54dd\n  Stored in directory: /root/.cache/pip/wheels/23/e8/f0/b3c58c12d1ffe60bcc8c7d121115f26b2c1878653edfca48db\nSuccessfully built transformers_stream_generator\nInstalling collected packages: transformers_stream_generator\nSuccessfully installed transformers_stream_generator-0.0.5\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import torch\n","\n","def format_prompt(query, context):\n","    prompt = f\"\"\"<|im_start|>system\n","Bạn là chuyên gia phân tích tài chính. Hãy trả lời câu hỏi và bạn có thể dựa trên thông tin sau hoặc không nếu không cần:\n","<|im_end|>\n","<|im_start|>user\n","Câu hỏi: {query.strip()}\n","\n","Thông tin tham khảo:\n","{context.strip()}\n","\n","Yêu cầu:\n","- Chỉ sử dụng thông tin tham khảo, không đề cập nó trong câu trả lời của bạn\n","- Trả lời tự nhiên, có logic\n","- Nếu không có thông tin, hãy tự trả lời theo cách của bạn\n","- Nếu không đủ thông tin để kết luận, hãy nói rõ\n","<|im_end|>\n","<|im_start|>assistant\n","\"\"\"\n","    return prompt\n","\n","def generate_response_from_qwen(query, context_chunks, max_new_tokens=512):\n","    # Prepare the prompt\n","    prompt = format_prompt(query, context_chunks)\n","    inputs = tokenizer_llm(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n","\n","    # Generating response with optimized settings for Qwen\n","    with torch.inference_mode():\n","        outputs = model_llm.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            temperature=0.7,\n","            do_sample=False,\n","        )\n","\n","    # Decoding and post-processing the response\n","    response = tokenizer_llm.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Cleaning up the response using a more robust method\n","    response_cleaned = clean_qwen_response(response)\n","\n","    return response_cleaned.strip()\n","\n","def clean_qwen_response(response):\n","    \"\"\"\n","    Clean the response generated by the Qwen model to ensure it only returns the assistant's answer.\n","    \"\"\"\n","    # Split by the assistant's section start marker\n","    if \"<|im_start|>assistant\" in response:\n","        response = response.split(\"<|im_start|>assistant\")[-1]\n","\n","    # Remove any remaining Qwen special tokens\n","    response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\")\n","\n","    # Remove any leading or trailing whitespace\n","    return response.strip()\n"],"metadata":{"id":"LEiTGEewBurx","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:07:50.668665Z","iopub.execute_input":"2025-05-09T04:07:50.669255Z","iopub.status.idle":"2025-05-09T04:07:50.675809Z","shell.execute_reply.started":"2025-05-09T04:07:50.669232Z","shell.execute_reply":"2025-05-09T04:07:50.675081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["context_chunks = format_prompt(query, source_information)\n","answer = generate_response_from_qwen(query, context_chunks, 512)\n","print(\"🧠 Câu trả lời từ Qwen:\")\n","print(answer)\n"],"metadata":{"id":"yxRU0aFN3Hwx","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T04:07:52.852352Z","iopub.execute_input":"2025-05-09T04:07:52.852982Z","iopub.status.idle":"2025-05-09T04:10:14.994094Z","shell.execute_reply.started":"2025-05-09T04:07:52.852960Z","shell.execute_reply":"2025-05-09T04:10:14.993383Z"},"outputId":"2ecd336f-f71f-4e22-aa6a-6a6c62bc33b8"},"outputs":[{"name":"stdout","text":"🧠 Câu trả lời từ Qwen:\nsystem\nBạn là chuyên gia phân tích tài chính. Hãy trả lời câu hỏi và bạn có thể dựa trên thông tin sau hoặc không nếu không cần:\n\nuser\nCâu hỏi: Lợi nhuận quý I/2024 của CMG có tăng không?\n\nThông tin tham khảo:\nsystem\nBạn là chuyên gia phân tích tài chính. Hãy trả lời câu hỏi và bạn có thể dựa trên thông tin sau hoặc không nếu không cần:\n\nuser\nCâu hỏi: Lợi nhuận quý I/2024 của CMG có tăng không?\n\nThông tin tham khảo:\nThông tin 1:\ntext               Loại giao dịch: GD CĐ lớn. Người thực hiện: Ge...\nticker                                                           CMG\nrecord_date                                      2024-03-26 00:00:00\ndate                                             2024-03-26 00:00:00\nsource                                                      internal\nembedding_index                                                 4239\nName: 4239, dtype: object\nThông tin 2:\ntext               Loại giao dịch: GD CĐ lớn. Người thực hiện: Ge...\nticker                                                           CMG\nrecord_date                                      2024-03-05 00:00:00\ndate                                             2024-03-05 00:00:00\nsource                                                      internal\nembedding_index                                                 4240\nName: 4240, dtype: object\nThông tin 3:\ntext                                 .\nticker                             FPT\nrecord_date        2024-05-09 00:00:00\ndate               2024-05-09 00:00:00\nsource                           cafef\nembedding_index                   3428\nName: 3428, dtype: object\nThông tin 4:\ntext                                 .\nticker                             FPT\nrecord_date        2024-06-18 00:00:00\ndate               2024-06-18 00:00:00\nsource                           cafef\nembedding_index                   2883\nName: 2883, dtype: object\nThông tin 5:\ntext                                 .\nticker                             FPT\nrecord_date        2024-06-21 00:00:00\ndate               2024-06-21 00:00:00\nsource                           cafef\nembedding_index                   2849\nName: 2849, dtype: object\nThông tin 6:\ntext                                 .\nticker                             FPT\nrecord_date        2024-08-20 00:00:00\ndate               2024-08-20 00:00:00\nsource                           cafef\nembedding_index                   2166\nName: 2166, dtype: object\nThông tin 7:\ntext                                 .\nticker                             FPT\nrecord_date        2024-08-28 00:00:00\ndate               2024-08-28 00:00:00\nsource                           cafef\nembedding_index                   2078\nName: 2078, dtype: object\nThông tin 8:\ntext               . Tỷ lệ sau giao dịch: 8.97%.\nticker                                       CMG\nrecord_date                  2023-08-29 00:00:00\ndate                         2023-08-29 00:00:00\nsource                                  internal\nembedding_index                             4253\nName: 4253, dtype: object\nThông tin 9:\ntext               . Số lượng sau giao dịch: 20,836,081. Tỷ lệ sa...\nticker                                                           FPT\nrecord_date                                      2023-07-24 00:00:00\ndate                                             2023-07-24 00:00:00\nsource                                                      internal\nembedding_index                                                 4189\nName: 4189, dtype: object\nThông tin 10:\ntext               . Số lượng sau giao dịch: 826,220. Tỷ lệ sau g...\nticker                                                           FPT\nrecord_date                                      2023-10-10 00:00:00\ndate                                             2023-10-10 00:00:00\nsource                                                      internal\nembedding_index                                                 4179\nName: 4179, dtype: object\n\nYêu cầu:\n- Chỉ sử dụng thông tin tham khảo, không đề cập nó trong câu trả lời của bạn\n- Trả lời tự nhiên, có logic\n- Nếu không có thông tin, hãy tự trả lời theo cách của bạn\n- Nếu không đủ thông tin để kết luận, hãy nói rõ\n\nassistant\n\nYêu cầu:\n- Chỉ sử dụng thông tin tham khảo, không đề cập nó trong câu trả lời của bạn\n- Trả lời tự nhiên, có logic\n- Nếu không có thông tin, hãy tự trả lời theo cách của bạn\n- Nếu không đủ thông tin để kết luận, hãy nói rõ\n\nassistant\nDựa trên thông tin tham khảo mà bạn cung cấp, chúng tôi không có dữ liệu cụ thể về lợi nhuận quý I/2024 của CMG. Thông tin tham khảo chủ yếu tập trung vào các giao dịch cổ phiếu và một số thông tin khác, nhưng không có dữ liệu về lợi nhuận hay báo cáo tài chính chi tiết. Do đó, chúng tôi không thể đưa ra kết luận chính xác về việc lợi nhuận quý I/2024 của CMG có tăng hay không.\n\nTuy nhiên, theo xu hướng chung, nếu CMG duy trì được sự tăng trưởng ổn định và duy trì hiệu quả quản lý chi phí, lợi nhuận của họ có thể tăng. Ngược lại, nếu có bất kỳ thách thức nào ảnh hưởng đến hoạt động kinh doanh, lợi nhuận có thể bị ảnh hưởng. Để có cái nhìn chính xác hơn, chúng tôi cần xem xét thêm các báo cáo tài chính chính thức và các thông tin khác liên quan.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Fine-tune RAG model"],"metadata":{"id":"gaBv2mmzVLLh"}},{"cell_type":"markdown","source":["## **1. Chuẩn bị dữ liệu train**"],"metadata":{"id":"K97EwcbCVLLh"}},{"cell_type":"code","source":["!pip install openai"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:14.536259Z","iopub.execute_input":"2025-05-09T03:27:14.536560Z","iopub.status.idle":"2025-05-09T03:27:17.628823Z","shell.execute_reply.started":"2025-05-09T03:27:14.536537Z","shell.execute_reply":"2025-05-09T03:27:17.627800Z"},"id":"nX02Kf5FVLLh","outputId":"4a886ab2-5568-44c5-9ef5-96f431709ac4"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import openai\n","\n","# Đảm bảo bạn đã thiết lập API Key của OpenAI trong biến môi trường\n","openai.api_key = \"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\"\n","\n","def generate_instruction(text):\n","    \"\"\"\n","    Gọi API OpenAI để tạo câu hỏi đọc hiểu dựa trên đoạn văn đầu vào.\n","    \"\"\"\n","    # Định dạng prompt cho API OpenAI\n","    prompt = f\"\"\"Bạn là một chuyên gia tạo câu hỏi đọc hiểu.\n","Tạo một câu hỏi ngắn, rõ ràng để kiểm tra khả năng hiểu đoạn văn sau:\n","{text}\n","\n","Câu hỏi là gì?\"\"\"\n","\n","    try:\n","        # Gọi API OpenAI với mô hình GPT-4 (có thể thay đổi thành mô hình khác)\n","        response = openai.chat.completions.create(\n","            model=\"gpt-4\",   # Thay đổi mô hình nếu bạn muốn (vd: gpt-3.5-turbo)\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"Bạn là một chuyên gia tạo câu hỏi đọc hiểu.\"},\n","                {\"role\": \"user\", \"content\": prompt}\n","            ],\n","            max_tokens=64,\n","            temperature=0.7,\n","            n=1,\n","            stop=None\n","        )\n","\n","        # Lấy câu trả lời từ API\n","        question = response.choices[0].message.content\n","        return question\n","\n","    except openai.error.OpenAIError as e:\n","        print(f\"Đã xảy ra lỗi khi gọi API OpenAI: {str(e)}\")\n","        return \"Lỗi khi gọi API OpenAI.\"\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:17.630217Z","iopub.execute_input":"2025-05-09T03:27:17.630501Z","iopub.status.idle":"2025-05-09T03:27:17.636928Z","shell.execute_reply.started":"2025-05-09T03:27:17.630477Z","shell.execute_reply":"2025-05-09T03:27:17.636159Z"},"id":"o08K8ME2VLLi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["train_data = []\n","\n","for _, row in df_chunks.iterrows():\n","    context = row['text']\n","    question = generate_instruction(context)\n","\n","    sample = {\n","        \"instruction\": question,\n","        \"input\": \"\",\n","        \"output\": context  # hoặc bạn sinh câu trả lời nếu muốn huấn luyện trả lời ngắn gọn\n","    }\n","    train_data.append(sample)\n","    print(sample)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:17.637601Z","iopub.execute_input":"2025-05-09T03:27:17.637907Z","iopub.status.idle":"2025-05-09T03:27:19.970258Z","shell.execute_reply.started":"2025-05-09T03:27:17.637882Z","shell.execute_reply":"2025-05-09T03:27:19.968835Z"},"id":"ybI1ULI4VLLi","outputId":"ff527d6b-eb39-4de3-ca79-a720da2a6f47"},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2998084384.py\u001b[0m in \u001b[0;36mgenerate_instruction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Gọi API OpenAI với mô hình GPT-4 (có thể thay đổi thành mô hình khác)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Thay đổi mô hình nếu bạn muốn (vd: gpt-3.5-turbo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2327156832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     sample = {\n","\u001b[0;32m/tmp/ipykernel_31/2998084384.py\u001b[0m in \u001b[0;36mgenerate_instruction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Đã xảy ra lỗi khi gọi API OpenAI: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Lỗi khi gọi API OpenAI.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"],"ename":"AttributeError","evalue":"module 'openai' has no attribute 'error'","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":["print(train_data)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.970743Z","iopub.status.idle":"2025-05-09T03:27:19.970956Z","shell.execute_reply.started":"2025-05-09T03:27:19.970854Z","shell.execute_reply":"2025-05-09T03:27:19.970863Z"},"id":"q0dtddejVLLi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import json\n","\n","with open(\"/kaggle/working/finetune_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n","    for example in train_data:\n","        f.write(json.dumps(example, ensure_ascii=False) + \"\\n\")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.971934Z","iopub.status.idle":"2025-05-09T03:27:19.972418Z","shell.execute_reply.started":"2025-05-09T03:27:19.972263Z","shell.execute_reply":"2025-05-09T03:27:19.972281Z"},"id":"uRm2OEpOVLLi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install transformers peft accelerate bitsandbytes torch datasets"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.973925Z","iopub.status.idle":"2025-05-09T03:27:19.974485Z","shell.execute_reply.started":"2025-05-09T03:27:19.974309Z","shell.execute_reply":"2025-05-09T03:27:19.974325Z"},"id":"lgu7glqCVLLi"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Train\n"],"metadata":{"id":"rxwe6G0kVLLi"}},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n","    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n","    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n","    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n","    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/gemma-2-9b-bnb-4bit\",\n","    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    # Can select any from the below:\n","    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n","    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n","    # And also all Instruct versions and Math. Coding verisons!\n","    model_name = \"unsloth/Qwen2.5-7B-Instruct\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.975212Z","iopub.status.idle":"2025-05-09T03:27:19.975419Z","shell.execute_reply.started":"2025-05-09T03:27:19.975315Z","shell.execute_reply":"2025-05-09T03:27:19.975324Z"},"id":"u7eh9H-wVLLi"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.976639Z","iopub.status.idle":"2025-05-09T03:27:19.976932Z","shell.execute_reply.started":"2025-05-09T03:27:19.976813Z","shell.execute_reply":"2025-05-09T03:27:19.976827Z"},"id":"z8hknez3VLLp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","from datasets import Dataset\n","import json\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","# from datasets import load_dataset\n","# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n","jsonl_file_path = '/kaggle/input/finetunerag/finetune_dataset.json'\n","with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n","    data = [json.loads(line) for line in file]\n","\n","dataset = Dataset.from_list(data)\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.978297Z","iopub.status.idle":"2025-05-09T03:27:19.978508Z","shell.execute_reply.started":"2025-05-09T03:27:19.978409Z","shell.execute_reply":"2025-05-09T03:27:19.978418Z"},"id":"HBBhe18OVLLp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 100,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.980321Z","iopub.status.idle":"2025-05-09T03:27:19.980659Z","shell.execute_reply.started":"2025-05-09T03:27:19.980497Z","shell.execute_reply":"2025-05-09T03:27:19.980512Z"},"id":"VKAOba86VLLq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# @title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.981573Z","iopub.status.idle":"2025-05-09T03:27:19.981833Z","shell.execute_reply.started":"2025-05-09T03:27:19.981704Z","shell.execute_reply":"2025-05-09T03:27:19.981718Z"},"id":"bPXjvXY-VLLq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainer_stats = trainer.train()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:27:19.983078Z","iopub.status.idle":"2025-05-09T03:27:19.983283Z","shell.execute_reply.started":"2025-05-09T03:27:19.983187Z","shell.execute_reply":"2025-05-09T03:27:19.983196Z"},"id":"qhjNCaowVLLq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Evaluation (Đánh giá mô hình)"],"metadata":{"id":"0O6SjqRGVLLq"}},{"cell_type":"code","source":["\n","# Đường dẫn tới tập dữ liệu đánh giá (JSONL)\n","jsonl_file_path = '/kaggle/input/finetunerag/finetune_dataset.json'\n","with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n","    data = [json.loads(line) for line in file]\n","\n","# Khởi tạo danh sách đánh giá\n","prompts = [entry['instruction'] for entry in data]\n","expected_answers = [entry['output'] for entry in data]\n","\n","# Khởi tạo metric BLEU và ROUGE\n","bleu_metric = load_metric(\"sacrebleu\")\n","rouge_metric = load_metric(\"rouge\")\n","\n","# Hàm sinh văn bản từ mô hình\n","def generate_response(prompt, max_tokens=100):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    with torch.cuda.amp.autocast():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=max_tokens,\n","            do_sample=False,\n","            num_beams=1,\n","            use_cache=True\n","        )\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Đánh giá mô hình\n","predictions = []\n","start_time = time.time()\n","\n","print(\"\\n🔍 Đang đánh giá mô hình...\")\n","for prompt, expected in zip(prompts, expected_answers):\n","    generated_answer = generate_response(prompt)\n","    predictions.append(generated_answer)\n","    print(f\"\\nPrompt: {prompt}\")\n","    print(f\"Expected Answer: {expected}\")\n","    print(f\"Generated Answer: {generated_answer}\")\n","\n","# Đo thời gian inference trung bình\n","end_time = time.time()\n","average_time = (end_time - start_time) / len(prompts)\n","print(f\"\\n Thời gian trung bình để sinh văn bản: {average_time:.4f} giây\")\n","\n","# Đánh giá BLEU và ROUGE\n","bleu_score = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in expected_answers])\n","rouge_score = rouge_metric.compute(predictions=predictions, references=expected_answers, rouge_types=[\"rouge2\"])[\"rouge2\"]\n","\n","# Đánh giá độ chính xác (Exact Match Accuracy)\n","accuracy = sum([1 if pred.strip().lower() == ref.strip().lower() else 0 for pred, ref in zip(predictions, expected_answers)]) / len(prompts)\n","\n","# In kết quả đánh giá\n","print(\"\\n Kết Quả Đánh Giá:\")\n","print(f\"BLEU Score: {bleu_score['score']:.2f}\")\n","print(f\"ROUGE-2 Score: {rouge_score['fmeasure']:.2f}\")\n","print(f\"Accuracy (Exact Match): {accuracy * 100:.2f}%\")\n","print(f\"Average Inference Time: {average_time:.4f} seconds\")\n","\n","# Lưu kết quả đánh giá vào file\n","evaluation_results = {\n","    \"BLEU Score\": bleu_score['score'],\n","    \"ROUGE-2 Score\": rouge_score['fmeasure'],\n","    \"Accuracy\": accuracy,\n","    \"Average Inference Time\": average_time\n","}\n","\n","with open(\"evaluation_results.json\", \"w\") as f:\n","    json.dump(evaluation_results, f, indent=4)\n","\n","print(\"\\n Đánh giá hoàn tất. Kết quả đã được lưu trong file 'evaluation_results.json'.\")"],"metadata":{"trusted":true,"id":"g3wI93W8VLLq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Lưu model"],"metadata":{"id":"pGGZI4v-VLLq"}},{"cell_type":"code","source":["model.save_pretrained(\"/kaggle/working/ragftqwen2\")  # Local saving\n","tokenizer.save_pretrained(\"/kaggle/working/ragftqwen2\")"],"metadata":{"trusted":true,"id":"Fim09VsyVLLq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["login vào huggingface\n","from huggingface_hub import login\n","login(token='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')"],"metadata":{"trusted":true,"id":"bjuwRTL8VLLq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","repo_id = \"KKcom0028/ragftqwen2\"  # Thay bằng username và tên repo bạn muốn\n","\n","api.create_repo(\n","    repo_id=repo_id,\n","    repo_type=\"model\",\n","    private=False,  # Đặt thành True nếu bạn muốn repo riêng tư\n","    exist_ok=True   # Không báo lỗi nếu repo đã tồn tại\n",")"],"metadata":{"trusted":true,"id":"4MA0Dr9YVLLq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","\n","api.upload_folder(\n","    folder_path=\"/kaggle/working/ragftqwen2\",\n","    repo_id=\"KKcom0028/ragftqwen2\",\n","    commit_message=\"Push fine-tuned model from Kaggle\",\n","    token=True\n",")"],"metadata":{"trusted":true,"id":"WztYy24RVLLq"},"outputs":[],"execution_count":null}]}